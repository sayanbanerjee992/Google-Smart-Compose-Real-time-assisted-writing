{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJMxf4z3fgDz"
   },
   "source": [
    "# Gmail Smart Compose: Real-Time Assisted Writing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vh-sNQHQfgD3"
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rCh-gCJ55eGJ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import email\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import*\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import nltk.translate.bleu_score as bleu\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow import keras \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import nltk\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "np.random.seed(42)\n",
    "import pickle\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BPlKN_Z35eGN"
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GjvhI5RH5eGN"
   },
   "outputs": [],
   "source": [
    "#Read the dataset\n",
    "\n",
    "df1 = pd.read_csv('final_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sVJ6b3HC5eGO",
    "outputId": "d1c5e832-656e-4427-f288-88421f2c1549"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3093422, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jgLMV0Qm5eGP",
    "outputId": "33f8df64-231f-417c-acba-b84bc97eefaa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dave, Here are the names of the west desk members by category.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The origination side is very sparse.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Phillip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Paula, million is fine Phillip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Forwarded by Phillip K Allen HOU ECT on PM Enron North America Corp. From Airam Arteaga PM To Phillip K Thomas A Scott John Grant Ted Vladimir Frank cc Rita Ina Laura Kimberly Araceli Kimberly Subject Var, Reporting and Resources Meeting Please plan to attend the below Meeting Topic Var, Reporting and Resources Meeting Date Wednesday, October th Time Location EB C If you have any questions conflicts, please feel free to call me.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                               body\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                    Dave, Here are the names of the west desk members by category.\n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                              The origination side is very sparse.\n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                           Phillip\n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                    Paula, million is fine Phillip\n",
       "4  Forwarded by Phillip K Allen HOU ECT on PM Enron North America Corp. From Airam Arteaga PM To Phillip K Thomas A Scott John Grant Ted Vladimir Frank cc Rita Ina Laura Kimberly Araceli Kimberly Subject Var, Reporting and Resources Meeting Please plan to attend the below Meeting Topic Var, Reporting and Resources Meeting Date Wednesday, October th Time Location EB C If you have any questions conflicts, please feel free to call me."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SLGhqGCo5eGP",
    "outputId": "9e3ddb6f-5c06-4036-b2f4-a5128bfdbf39"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I am randomly picked 50000 values, because of memory issue\n",
    "\n",
    "df= df1.sample(n = 50000)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZGopHA0y5eGQ",
    "outputId": "ec937e83-314a-4cae-c9a1-8882315b6510"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2026669</th>\n",
       "      <td>Your input i.e., a Self assessment, if you are a participant in this program, Manager assessment, Direct Report assessment, or Colleague Peer assessment will be combined with the input of others and used by the program participant to develop an action plan to improve his her management styles and practices.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985088</th>\n",
       "      <td>I also am including a draft of an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731740</th>\n",
       "      <td>Tim's e mail address is Let me know if I can get you any specific information and I'll do my best!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2201491</th>\n",
       "      <td>BGE defines the Scheduling Coordinator as \"An entity recognized the the PJM OI and qualified to act on behalf of the Electricity Supplier in taking such actions with PJM as are necessary in order for Electricity Supplier's obligations as defined in this Tariff to be met, including the submission of energy schedules to the PJM OI.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2621496</th>\n",
       "      <td>This e mail and any attachments hereto are not intended to be an offer or an acceptance and do not create or evidence a binding and enforceable contract between Enron Corp. or any of its affiliates and the intended recipient or any other party, and may not be relied on by anyone as the basis of a contract by estoppel or otherwise.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                 body\n",
       "2026669                          Your input i.e., a Self assessment, if you are a participant in this program, Manager assessment, Direct Report assessment, or Colleague Peer assessment will be combined with the input of others and used by the program participant to develop an action plan to improve his her management styles and practices.\n",
       "985088                                                                                                                                                                                        I also am including a draft of an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.\n",
       "731740                                                                                                                                                                                                                                             Tim's e mail address is Let me know if I can get you any specific information and I'll do my best!\n",
       "2201491   BGE defines the Scheduling Coordinator as \"An entity recognized the the PJM OI and qualified to act on behalf of the Electricity Supplier in taking such actions with PJM as are necessary in order for Electricity Supplier's obligations as defined in this Tariff to be met, including the submission of energy schedules to the PJM OI.\n",
       "2621496  This e mail and any attachments hereto are not intended to be an offer or an acceptance and do not create or evidence a binding and enforceable contract between Enron Corp. or any of its affiliates and the intended recipient or any other party, and may not be relied on by anyone as the basis of a contract by estoppel or otherwise."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WmEVfRnx5eGQ"
   },
   "outputs": [],
   "source": [
    "del df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3MP6OJNf5eGR",
    "outputId": "6adc532c-123f-4646-c4a0-148a5f010af8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th- percentile of len(text) is 1.0\n",
      "========================================\n",
      "10th- percentile of len(text) is 12.0\n",
      "========================================\n",
      "20th- percentile of len(text) is 27.0\n",
      "========================================\n",
      "30th- percentile of len(text) is 41.0\n",
      "========================================\n",
      "40th- percentile of len(text) is 55.0\n",
      "========================================\n",
      "50th- percentile of len(text) is 70.0\n",
      "========================================\n",
      "60th- percentile of len(text) is 88.0\n",
      "========================================\n",
      "70th- percentile of len(text) is 110.0\n",
      "========================================\n",
      "80th- percentile of len(text) is 140.0\n",
      "========================================\n",
      "90th- percentile of len(text) is 189.0\n",
      "========================================\n",
      "100th- percentile of len(text) is 2439.0\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "\n",
    "x = list(df['body'].apply(lambda x:len(x)))\n",
    "\n",
    "for i in range(0,110,10):\n",
    "    print(\"{}th- percentile of len(text) is {}\".format(i,np.percentile(x, i)))\n",
    "    print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FIehZvtU5eGR"
   },
   "outputs": [],
   "source": [
    "df['length'] = list(df['body'].apply(lambda x:len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0znaq7EG5eGS",
    "outputId": "ccecaa94-4838-4c4f-b96e-9cc429e28559"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2026669</th>\n",
       "      <td>Your input i.e., a Self assessment, if you are a participant in this program, Manager assessment, Direct Report assessment, or Colleague Peer assessment will be combined with the input of others and used by the program participant to develop an action plan to improve his her management styles and practices.</td>\n",
       "      <td>308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985088</th>\n",
       "      <td>I also am including a draft of an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731740</th>\n",
       "      <td>Tim's e mail address is Let me know if I can get you any specific information and I'll do my best!</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2201491</th>\n",
       "      <td>BGE defines the Scheduling Coordinator as \"An entity recognized the the PJM OI and qualified to act on behalf of the Electricity Supplier in taking such actions with PJM as are necessary in order for Electricity Supplier's obligations as defined in this Tariff to be met, including the submission of energy schedules to the PJM OI.</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2621496</th>\n",
       "      <td>This e mail and any attachments hereto are not intended to be an offer or an acceptance and do not create or evidence a binding and enforceable contract between Enron Corp. or any of its affiliates and the intended recipient or any other party, and may not be relied on by anyone as the basis of a contract by estoppel or otherwise.</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                 body  \\\n",
       "2026669                          Your input i.e., a Self assessment, if you are a participant in this program, Manager assessment, Direct Report assessment, or Colleague Peer assessment will be combined with the input of others and used by the program participant to develop an action plan to improve his her management styles and practices.   \n",
       "985088                                                                                                                                                                                        I also am including a draft of an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.   \n",
       "731740                                                                                                                                                                                                                                             Tim's e mail address is Let me know if I can get you any specific information and I'll do my best!   \n",
       "2201491   BGE defines the Scheduling Coordinator as \"An entity recognized the the PJM OI and qualified to act on behalf of the Electricity Supplier in taking such actions with PJM as are necessary in order for Electricity Supplier's obligations as defined in this Tariff to be met, including the submission of energy schedules to the PJM OI.   \n",
       "2621496  This e mail and any attachments hereto are not intended to be an offer or an acceptance and do not create or evidence a binding and enforceable contract between Enron Corp. or any of its affiliates and the intended recipient or any other party, and may not be relied on by anyone as the basis of a contract by estoppel or otherwise.   \n",
       "\n",
       "         length  \n",
       "2026669     308  \n",
       "985088      151  \n",
       "731740       98  \n",
       "2201491     331  \n",
       "2621496     332  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nglfOFPZ5eGS",
    "outputId": "5f979f18-2f5c-4661-af50-9ee8cffde09f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fxNG5shU5eGS",
    "outputId": "e7e46e73-9a77-453b-9090-b8dfbeff1193"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44876"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop all the rows, lendth more than 200 and less than 1\n",
    "df = df[df['length']<=200]\n",
    "\n",
    "df = df[df['length']>2]\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xP_Yp0us5eGT",
    "outputId": "e3e9a4d8-19b2-468d-a122-eea1845b1bc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th- percentile of len(text) is 3.0\n",
      "========================================\n",
      "10th- percentile of len(text) is 13.0\n",
      "========================================\n",
      "20th- percentile of len(text) is 26.0\n",
      "========================================\n",
      "30th- percentile of len(text) is 39.0\n",
      "========================================\n",
      "40th- percentile of len(text) is 51.0\n",
      "========================================\n",
      "50th- percentile of len(text) is 64.0\n",
      "========================================\n",
      "60th- percentile of len(text) is 80.0\n",
      "========================================\n",
      "70th- percentile of len(text) is 97.0\n",
      "========================================\n",
      "80th- percentile of len(text) is 119.0\n",
      "========================================\n",
      "90th- percentile of len(text) is 148.0\n",
      "========================================\n",
      "100th- percentile of len(text) is 200.0\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "\n",
    "x = list(df['body'].apply(lambda x:len(x)))\n",
    "\n",
    "for i in range(0,110,10):\n",
    "    print(\"{}th- percentile of len(text) is {}\".format(i,np.percentile(x, i)))\n",
    "    print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ev3Jivt5eGT",
    "outputId": "40f41341-84b3-4da7-8dbf-9a6e5796a386"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>985088</th>\n",
       "      <td>I also am including a draft of an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731740</th>\n",
       "      <td>Tim's e mail address is Let me know if I can get you any specific information and I'll do my best!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544444</th>\n",
       "      <td>Change Holst and Grigsby to Million and Million.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262374</th>\n",
       "      <td>We are presently planning our training engagements for the year.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689443</th>\n",
       "      <td>We have provided a list of the most common functions in this file, but if you need to add functions for clarity, please do so.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                            body\n",
       "985088   I also am including a draft of an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.\n",
       "731740                                                        Tim's e mail address is Let me know if I can get you any specific information and I'll do my best!\n",
       "1544444                                                                                                         Change Holst and Grigsby to Million and Million.\n",
       "1262374                                                                                         We are presently planning our training engagements for the year.\n",
       "689443                            We have provided a list of the most common functions in this file, but if you need to add functions for clarity, please do so."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.drop(['length'], axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "814OZxQZ5eGT",
    "outputId": "4c3dcc21-0be5-4800-9734-546d3b1753aa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2921960</th>\n",
       "      <td>Hey guys!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1404835</th>\n",
       "      <td>Jim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1531358</th>\n",
       "      <td>Sheila Can take the current spreadsheet attached and add the origination prc resuslts under the proposed cash bonus categy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098986</th>\n",
       "      <td>Services include the Americas, Asia Pacific, Europe CIS and the Middle East.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2153195</th>\n",
       "      <td>Monthly and daily prices and their spreads to the Hub would be nice.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                body\n",
       "2921960                                                                                                                    Hey guys!\n",
       "1404835                                                                                                                          Jim\n",
       "1531358  Sheila Can take the current spreadsheet attached and add the origination prc resuslts under the proposed cash bonus categy.\n",
       "1098986                                                 Services include the Americas, Asia Pacific, Europe CIS and the Middle East.\n",
       "2153195                                                         Monthly and daily prices and their spreads to the Hub would be nice."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2dzfA4B95eGU"
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmtgh1XS5eGU"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lpvXq88F5eGV"
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_sequence_data(emails):\n",
    "    '''\n",
    "    To convert our data into encoder-decoder sequences we split sentences between contiguous spans of words.    \n",
    "    sentences with < 5 words are dropped\n",
    "    '''\n",
    "    enc_seq = []\n",
    "    dec_seq = []\n",
    "    body= []\n",
    "    contents = emails['body']\n",
    "\n",
    "    for text in contents:\n",
    "        sent = text.split()\n",
    "        \n",
    "        for i in range(5,len(sent)):\n",
    "            enc_seq.append(' '.join(sent[:i]))\n",
    "            dec_seq.append(' '.join(sent[i:]))\n",
    "\n",
    "    # put all in a dataframe\n",
    "    data = pd.DataFrame()\n",
    "    data['body_enc_seq'] = enc_seq\n",
    "    data['body_dec_seq'] = dec_seq\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t1cJG_Ii5eGW",
    "outputId": "2abedf4d-3098-4367-dca7-1aacc6a2918d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_enc_seq</th>\n",
       "      <th>body_dec_seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I also am including a</td>\n",
       "      <td>draft of an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I also am including a draft</td>\n",
       "      <td>of an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I also am including a draft of</td>\n",
       "      <td>an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I also am including a draft of an</td>\n",
       "      <td>announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I also am including a draft of an announcement</td>\n",
       "      <td>to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     body_enc_seq  \\\n",
       "0                           I also am including a   \n",
       "1                     I also am including a draft   \n",
       "2                  I also am including a draft of   \n",
       "3               I also am including a draft of an   \n",
       "4  I also am including a draft of an announcement   \n",
       "\n",
       "                                                                                                                        body_dec_seq  \n",
       "0  draft of an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.  \n",
       "1        of an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.  \n",
       "2           an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.  \n",
       "3              announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.  \n",
       "4                           to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_data = generate_sequence_data(df)\n",
    "\n",
    "sequence_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5jx7RH195eGW"
   },
   "outputs": [],
   "source": [
    "# drop empty decoder-sequences\n",
    "sequence_data = sequence_data[sequence_data['body_dec_seq'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBf8Cdew5eGW"
   },
   "outputs": [],
   "source": [
    "#Adding <sos> as Start and <eos> as End\n",
    "\n",
    "sequence_data['body_dec_seq_inp'] = '<sos> '+sequence_data['body_dec_seq']\n",
    "sequence_data['body_dec_seq_out'] = sequence_data['body_dec_seq'] + ' <eos>'\n",
    "\n",
    "sequence_data['body_enc_seq'] = sequence_data['body_enc_seq'].apply(lambda x: str(x))\n",
    "sequence_data['body_enc_seq'] = '<sos> '+sequence_data['body_enc_seq']+' <eos>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3QBMwq25eGX",
    "outputId": "8e587973-778a-4ea8-8857-a09c0fad0485"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_enc_seq</th>\n",
       "      <th>body_dec_seq</th>\n",
       "      <th>body_dec_seq_inp</th>\n",
       "      <th>body_dec_seq_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;sos&gt; I also am including a &lt;eos&gt;</td>\n",
       "      <td>draft of an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.</td>\n",
       "      <td>&lt;sos&gt; draft of an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.</td>\n",
       "      <td>draft of an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively. &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;sos&gt; I also am including a draft &lt;eos&gt;</td>\n",
       "      <td>of an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.</td>\n",
       "      <td>&lt;sos&gt; of an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.</td>\n",
       "      <td>of an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively. &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;sos&gt; I also am including a draft of &lt;eos&gt;</td>\n",
       "      <td>an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.</td>\n",
       "      <td>&lt;sos&gt; an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.</td>\n",
       "      <td>an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively. &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;sos&gt; I also am including a draft of an &lt;eos&gt;</td>\n",
       "      <td>announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.</td>\n",
       "      <td>&lt;sos&gt; announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.</td>\n",
       "      <td>announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively. &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;sos&gt; I also am including a draft of an announcement &lt;eos&gt;</td>\n",
       "      <td>to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.</td>\n",
       "      <td>&lt;sos&gt; to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.</td>\n",
       "      <td>to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively. &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 body_enc_seq  \\\n",
       "0                           <sos> I also am including a <eos>   \n",
       "1                     <sos> I also am including a draft <eos>   \n",
       "2                  <sos> I also am including a draft of <eos>   \n",
       "3               <sos> I also am including a draft of an <eos>   \n",
       "4  <sos> I also am including a draft of an announcement <eos>   \n",
       "\n",
       "                                                                                                                        body_dec_seq  \\\n",
       "0  draft of an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.   \n",
       "1        of an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.   \n",
       "2           an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.   \n",
       "3              announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.   \n",
       "4                           to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.   \n",
       "\n",
       "                                                                                                                          body_dec_seq_inp  \\\n",
       "0  <sos> draft of an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.   \n",
       "1        <sos> of an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.   \n",
       "2           <sos> an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.   \n",
       "3              <sos> announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.   \n",
       "4                           <sos> to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.   \n",
       "\n",
       "                                                                                                                          body_dec_seq_out  \n",
       "0  draft of an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively. <eos>  \n",
       "1        of an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively. <eos>  \n",
       "2           an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively. <eos>  \n",
       "3              announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively. <eos>  \n",
       "4                           to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively. <eos>  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oAYri-LD5eGX",
    "outputId": "47759b26-f165-4c0c-f779-ed9af7d3d232"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(389673, 4)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zwg9nbfs5eGX"
   },
   "outputs": [],
   "source": [
    "# save data\n",
    "import pickle\n",
    "\n",
    "with open('Sequence_data.pickle', 'wb') as file:\n",
    "    pickle.dump(sequence_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZYf78V6MgIef",
    "outputId": "47189858-d087-4c13-bf3a-a6a0a58b8348"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-08-10 16:14:51--  https://doc-04-a8-docs.googleusercontent.com/docs/securesc/9b2j1ftvtidp8f9e843gisfckled5hmm/nnf6rdqigkpasqep996o9hsprr5ssvif/1660148025000/01806599350694168671/01806599350694168671/1ZNZcR9XwJq3nMdrT62CmkJCC4OyMCCqR?e=download&ax=AI9vYm5pbOyYvdQYY_mxp6ixRyYlXjYYqqcFft9j_qWxveVR6CMj6XIEVKCXhKn8VltC_EfNb_0P59_zPyqCBSTifnCQIlqS9qBC6kyq5okPQDBwC09XeVAtUXNSnnXrRIl_HXlh8eFGHr7zhQVAC2rpKCUsNLoAB7eQhhZLkdjbDqt3nAuFlqtSt9spEIspFxplKBaKVrfr7BPtsRaE2fOaSJHOQ2lLv5DtpbUlb4t3kp_bsQ3ChPNztrX09L9kLpa_MR6gODBM-_Iapw3zp9VDdGPIp9nk845pZ6-1rCpxiAacOPbFDXjYex_e9gjJ0fYrgANm2nXgRcmjGBBShRIyAWeY0paUVD0foUVips8RtUv8olVh9GLh8JCFrnS7EuMBZxwQ9aHusUiEfiFYA0U_kl6sWZsAAELrTzqhnYGJsyNhtyHd_Ng20EVnxNSfNtCZrgpaUE11-6Zl1jUIpZSAya5e6z6z8JhGVjcBJ9TpkLMu0QDI4C0KFwh6vkHR29-3UhkltzBgtmVStDdAN_PGW4DeBBeI0PIaXqVx_kOnPZupsAd4TWcZT_GSYz-hj-LVGVhUrLcmMMLi2hiTY_8RS0Mnk6TsJMzC4iP_H4fbgw_ZEOOcUE8PQ57cnP3daohTiARhh9qzwD6HDsPATtya71y51mBAk2jYNyQO8ymuiOIEI3rHyz8U9SSYy7RHc-N4te8gvNL_l3sSB2t5&uuid=962bc8de-5d21-4f99-a5c9-c0d10d0bfb62&authuser=0&nonce=c3a5mqkteefqm&user=01806599350694168671&hash=7rt0vh4qb7mj2k382s27p7psud53cgbk\n",
      "Resolving doc-04-a8-docs.googleusercontent.com (doc-04-a8-docs.googleusercontent.com)... 173.194.74.132, 2607:f8b0:4001:c0d::84\n",
      "Connecting to doc-04-a8-docs.googleusercontent.com (doc-04-a8-docs.googleusercontent.com)|173.194.74.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 101363371 (97M) [application/octet-stream]\n",
      "Saving to: ‘Sequence_data.pickle’\n",
      "\n",
      "Sequence_data.pickl 100%[===================>]  96.67M   138MB/s    in 0.7s    \n",
      "\n",
      "2022-08-10 16:14:52 (138 MB/s) - ‘Sequence_data.pickle’ saved [101363371/101363371]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --header=\"Host: doc-04-a8-docs.googleusercontent.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-US,en;q=0.9\" --header=\"Cookie: AUTH_i8e72o951agrsp03g5i063at61npposo_nonce=c3a5mqkteefqm\" --header=\"Connection: keep-alive\" \"https://doc-04-a8-docs.googleusercontent.com/docs/securesc/9b2j1ftvtidp8f9e843gisfckled5hmm/nnf6rdqigkpasqep996o9hsprr5ssvif/1660148025000/01806599350694168671/01806599350694168671/1ZNZcR9XwJq3nMdrT62CmkJCC4OyMCCqR?e=download&ax=AI9vYm5pbOyYvdQYY_mxp6ixRyYlXjYYqqcFft9j_qWxveVR6CMj6XIEVKCXhKn8VltC_EfNb_0P59_zPyqCBSTifnCQIlqS9qBC6kyq5okPQDBwC09XeVAtUXNSnnXrRIl_HXlh8eFGHr7zhQVAC2rpKCUsNLoAB7eQhhZLkdjbDqt3nAuFlqtSt9spEIspFxplKBaKVrfr7BPtsRaE2fOaSJHOQ2lLv5DtpbUlb4t3kp_bsQ3ChPNztrX09L9kLpa_MR6gODBM-_Iapw3zp9VDdGPIp9nk845pZ6-1rCpxiAacOPbFDXjYex_e9gjJ0fYrgANm2nXgRcmjGBBShRIyAWeY0paUVD0foUVips8RtUv8olVh9GLh8JCFrnS7EuMBZxwQ9aHusUiEfiFYA0U_kl6sWZsAAELrTzqhnYGJsyNhtyHd_Ng20EVnxNSfNtCZrgpaUE11-6Zl1jUIpZSAya5e6z6z8JhGVjcBJ9TpkLMu0QDI4C0KFwh6vkHR29-3UhkltzBgtmVStDdAN_PGW4DeBBeI0PIaXqVx_kOnPZupsAd4TWcZT_GSYz-hj-LVGVhUrLcmMMLi2hiTY_8RS0Mnk6TsJMzC4iP_H4fbgw_ZEOOcUE8PQ57cnP3daohTiARhh9qzwD6HDsPATtya71y51mBAk2jYNyQO8ymuiOIEI3rHyz8U9SSYy7RHc-N4te8gvNL_l3sSB2t5&uuid=962bc8de-5d21-4f99-a5c9-c0d10d0bfb62&authuser=0&nonce=c3a5mqkteefqm&user=01806599350694168671&hash=7rt0vh4qb7mj2k382s27p7psud53cgbk\" -c -O 'Sequence_data.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WmabqDocgIyJ",
    "outputId": "a0c769bc-2981-4618-8f83-919fec68fe43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-08-10 16:15:20--  https://doc-10-a8-docs.googleusercontent.com/docs/securesc/9b2j1ftvtidp8f9e843gisfckled5hmm/vfmkelat40acsv7csncsgkncpu5616k6/1660148100000/01806599350694168671/01806599350694168671/1Ix-I-jJDPL0WXJLHDAnmaNdElXwLrdkO?e=download&ax=AI9vYm42EuivGXOIc_dX3LkH6gHA5CUsxa-SZYwVzlqO7vDRh5busc0gUQVIFGBPMCxZzpEjJBcgiJCoiFHpm7oBSa6weoFadJX9kJTovKHNTGbffFGgRWAiR_2qVahjIV42jtnzVt1fAl43lOKlZYG8HdY7KCGmK_96HKVU3YcQGapXN4jFMS-k98Va6L8ZsoVT0j5kIK7_35zLBkDB4scjA-AY6M7VMWqsESZE9oKfho3nuCdsQtpNlr4C2v6EjQxxTR7ypFw-rXUKaSMHX50qvYY3ZITnycQ5FSE_uOhNWyv2HOOOafGJ2rImpZ20oMKPnLymqGHMYQoP9S4YIwYHhHo4Jk1_sxu_b2YqBrz_BYtb49BC_-KWWnTXRwJquQ7LP8ckxyCXRG2KEybCruU6jxR5hHQvtHOfIbAs2nbXWoo6BScPzAgF3z_WbvdyQzSFLsSFtRByohNaNVAyHoWUa6VwYG3_fdZ1oF3ynPt7vuDC2rlZWgP0uwBBrFBMHGY6PvwofT9zCZC2xCFBDAKs8AxDZPAcUMbxnOpqFoFt-hBezyn583k3puFsZmVdm1693RT7z63imWjpoFh5wT6DSljbit0V4TaLfqf_tTyvycDBwVttOVdItLejaBeCemdrfT1VKU0RZmqRXTmmgvX4OOhuaisjX_BqE3q5zCI_lqHdvByG9DXz5ykPH2JcgFay-AQR70ZKswJnYIhK&uuid=ceaad859-5315-4db4-964b-3dd9129ca190&authuser=0\n",
      "Resolving doc-10-a8-docs.googleusercontent.com (doc-10-a8-docs.googleusercontent.com)... 173.194.74.132, 2607:f8b0:4001:c0d::84\n",
      "Connecting to doc-10-a8-docs.googleusercontent.com (doc-10-a8-docs.googleusercontent.com)|173.194.74.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1037962819 (990M) [text/plain]\n",
      "Saving to: ‘glove.6B.300d.txt’\n",
      "\n",
      "glove.6B.300d.txt   100%[===================>] 989.88M  97.3MB/s    in 8.5s    \n",
      "\n",
      "2022-08-10 16:15:29 (116 MB/s) - ‘glove.6B.300d.txt’ saved [1037962819/1037962819]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --header=\"Host: doc-10-a8-docs.googleusercontent.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-US,en;q=0.9\" --header=\"Cookie: AUTH_i8e72o951agrsp03g5i063at61npposo=01806599350694168671|1660148025000|952h82i1o7v5f07gp06bqbse7bi5db0t\" --header=\"Connection: keep-alive\" \"https://doc-10-a8-docs.googleusercontent.com/docs/securesc/9b2j1ftvtidp8f9e843gisfckled5hmm/vfmkelat40acsv7csncsgkncpu5616k6/1660148100000/01806599350694168671/01806599350694168671/1Ix-I-jJDPL0WXJLHDAnmaNdElXwLrdkO?e=download&ax=AI9vYm42EuivGXOIc_dX3LkH6gHA5CUsxa-SZYwVzlqO7vDRh5busc0gUQVIFGBPMCxZzpEjJBcgiJCoiFHpm7oBSa6weoFadJX9kJTovKHNTGbffFGgRWAiR_2qVahjIV42jtnzVt1fAl43lOKlZYG8HdY7KCGmK_96HKVU3YcQGapXN4jFMS-k98Va6L8ZsoVT0j5kIK7_35zLBkDB4scjA-AY6M7VMWqsESZE9oKfho3nuCdsQtpNlr4C2v6EjQxxTR7ypFw-rXUKaSMHX50qvYY3ZITnycQ5FSE_uOhNWyv2HOOOafGJ2rImpZ20oMKPnLymqGHMYQoP9S4YIwYHhHo4Jk1_sxu_b2YqBrz_BYtb49BC_-KWWnTXRwJquQ7LP8ckxyCXRG2KEybCruU6jxR5hHQvtHOfIbAs2nbXWoo6BScPzAgF3z_WbvdyQzSFLsSFtRByohNaNVAyHoWUa6VwYG3_fdZ1oF3ynPt7vuDC2rlZWgP0uwBBrFBMHGY6PvwofT9zCZC2xCFBDAKs8AxDZPAcUMbxnOpqFoFt-hBezyn583k3puFsZmVdm1693RT7z63imWjpoFh5wT6DSljbit0V4TaLfqf_tTyvycDBwVttOVdItLejaBeCemdrfT1VKU0RZmqRXTmmgvX4OOhuaisjX_BqE3q5zCI_lqHdvByG9DXz5ykPH2JcgFay-AQR70ZKswJnYIhK&uuid=ceaad859-5315-4db4-964b-3dd9129ca190&authuser=0\" -c -O 'glove.6B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mf0SXFFF5eGY"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "import pickle\n",
    "\n",
    "with open('Sequence_data.pickle', 'rb') as file:\n",
    "    sequence_data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VHcz3l2x5eGY",
    "outputId": "d99d06c9-4bc2-40ff-b0cd-98d467962ab5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(389673, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "zz8K3gUY5eGY",
    "outputId": "689c583c-c78a-4656-8a53-624aa33d9a17"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-d567810f-e6c5-4fb2-a718-f2697fed481e\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_enc_seq</th>\n",
       "      <th>body_dec_seq</th>\n",
       "      <th>body_dec_seq_inp</th>\n",
       "      <th>body_dec_seq_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;sos&gt; I also am including a &lt;eos&gt;</td>\n",
       "      <td>draft of an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.</td>\n",
       "      <td>&lt;sos&gt; draft of an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.</td>\n",
       "      <td>draft of an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively. &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;sos&gt; I also am including a draft &lt;eos&gt;</td>\n",
       "      <td>of an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.</td>\n",
       "      <td>&lt;sos&gt; of an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.</td>\n",
       "      <td>of an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively. &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;sos&gt; I also am including a draft of &lt;eos&gt;</td>\n",
       "      <td>an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.</td>\n",
       "      <td>&lt;sos&gt; an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.</td>\n",
       "      <td>an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively. &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;sos&gt; I also am including a draft of an &lt;eos&gt;</td>\n",
       "      <td>announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.</td>\n",
       "      <td>&lt;sos&gt; announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.</td>\n",
       "      <td>announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively. &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;sos&gt; I also am including a draft of an announcement &lt;eos&gt;</td>\n",
       "      <td>to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.</td>\n",
       "      <td>&lt;sos&gt; to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.</td>\n",
       "      <td>to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively. &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d567810f-e6c5-4fb2-a718-f2697fed481e')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-d567810f-e6c5-4fb2-a718-f2697fed481e button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-d567810f-e6c5-4fb2-a718-f2697fed481e');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                                 body_enc_seq  \\\n",
       "0                           <sos> I also am including a <eos>   \n",
       "1                     <sos> I also am including a draft <eos>   \n",
       "2                  <sos> I also am including a draft of <eos>   \n",
       "3               <sos> I also am including a draft of an <eos>   \n",
       "4  <sos> I also am including a draft of an announcement <eos>   \n",
       "\n",
       "                                                                                                                        body_dec_seq  \\\n",
       "0  draft of an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.   \n",
       "1        of an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.   \n",
       "2           an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.   \n",
       "3              announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.   \n",
       "4                           to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.   \n",
       "\n",
       "                                                                                                                          body_dec_seq_inp  \\\n",
       "0  <sos> draft of an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.   \n",
       "1        <sos> of an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.   \n",
       "2           <sos> an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.   \n",
       "3              <sos> announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.   \n",
       "4                           <sos> to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively.   \n",
       "\n",
       "                                                                                                                          body_dec_seq_out  \n",
       "0  draft of an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively. <eos>  \n",
       "1        of an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively. <eos>  \n",
       "2           an announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively. <eos>  \n",
       "3              announcement to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively. <eos>  \n",
       "4                           to Omaha and Houston employees regarding the meetings you have scheduled on the th and th, respectively. <eos>  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7MAbDxnj5eGY"
   },
   "outputs": [],
   "source": [
    "#train,cv,test split\n",
    "\n",
    "train, test = train_test_split(sequence_data, test_size = 0.2, random_state = 0)\n",
    "\n",
    "train, val = train_test_split(train, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9NRH5uno5eGY"
   },
   "outputs": [],
   "source": [
    "#Tokenization is breaking the raw text into small chunks.\n",
    "\n",
    "tokenizer_enc = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
    "\n",
    "tokenizer_enc.fit_on_texts(train['body_enc_seq'].values)\n",
    "\n",
    "\n",
    "tokenizer_dec = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
    "\n",
    "tokenizer_dec.fit_on_texts(train['body_dec_seq_inp'].values + train['body_dec_seq_out'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SyVWtiCJ5eGY"
   },
   "outputs": [],
   "source": [
    "#gathering some other data\n",
    "\n",
    "max_enc = 0\n",
    "enc_vocab = set()\n",
    "for sent in sequence_data['body_dec_seq_inp'].values+sequence_data['body_dec_seq_out'].values:\n",
    "    length = len(sent.split())\n",
    "    if length > max_enc:\n",
    "        max_enc = length\n",
    "    for word in sent.split():\n",
    "        enc_vocab.add(word)\n",
    "vocab_size_enc = len(enc_vocab)\n",
    "\n",
    "max_dec = 0\n",
    "dec_vocab = set()\n",
    "for sent in sequence_data['body_enc_seq']:\n",
    "    length = len(sent.split())\n",
    "    if length > max_dec:\n",
    "        max_dec = length\n",
    "    for word in sent.split():\n",
    "        dec_vocab.add(word)\n",
    "vocab_size_dec = len(dec_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "dlZhpzTv5eGZ"
   },
   "outputs": [],
   "source": [
    "#Creating data for encoder decoder model\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, data, tokenizer_enc, tokenizer_dec, max_enc, max_dec):\n",
    "        self.encoder_inps = data['body_enc_seq'].values\n",
    "        self.decoder_inps = data['body_dec_seq_inp'].values\n",
    "        self.decoder_outs = data['body_dec_seq_out'].values\n",
    "        self.tokenizer_dec = tokenizer_dec\n",
    "        self.tokenizer_enc = tokenizer_enc\n",
    "        self.max_enc = max_enc\n",
    "        self.max_dec = max_dec\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        self.encoder_seq = self.tokenizer_enc.texts_to_sequences([self.encoder_inps[i]]) # need to pass list of values\n",
    "        self.decoder_inp_seq = self.tokenizer_dec.texts_to_sequences([self.decoder_inps[i]])\n",
    "        self.decoder_out_seq = self.tokenizer_dec.texts_to_sequences([self.decoder_outs[i]])\n",
    "\n",
    "        self.encoder_seq = pad_sequences(self.encoder_seq, maxlen=self.max_enc, dtype='int32', padding='post')\n",
    "        self.decoder_inp_seq = pad_sequences(self.decoder_inp_seq, maxlen=self.max_dec, dtype='int32', padding='post')\n",
    "        self.decoder_out_seq = pad_sequences(self.decoder_out_seq, maxlen=self.max_dec, dtype='int32', padding='post')\n",
    "        return self.encoder_seq, self.decoder_inp_seq, self.decoder_out_seq\n",
    "\n",
    "    def __len__(self): # your model.fit_gen requires this function\n",
    "        return len(self.encoder_inps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "FsTZETAY5eGZ"
   },
   "outputs": [],
   "source": [
    "#Creating batch for encoder decoder model\n",
    "\n",
    "class Dataloder(tf.keras.utils.Sequence):    \n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.indexes = np.arange(len(self.dataset.encoder_inps))\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        start = i * self.batch_size\n",
    "        stop = (i + 1) * self.batch_size\n",
    "        data = []\n",
    "        for j in range(start, stop):\n",
    "            data.append(self.dataset[j])\n",
    "\n",
    "        batch = [np.squeeze(np.stack(samples, axis=1), axis=0) for samples in zip(*data)]\n",
    "\n",
    "        return tuple([[tf.convert_to_tensor(batch[0]), tf.convert_to_tensor(batch[1])], tf.convert_to_tensor(batch[2])])\n",
    "\n",
    "    \n",
    "    def __len__(self):  \n",
    "        # your model.fit_gen requires this function\n",
    "        return len(self.indexes) // self.batch_size\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \n",
    "        self.indexes = np.random.permutation(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tWhFZBuk5eGZ",
    "outputId": "1ed9fe97-ef57-493e-a848-22ff567dbf3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#word embedding is a learned representation for text where words that have the same meaning have a similar representation.\n",
    "\n",
    "embeddings_index = dict()\n",
    "f = open('glove.6B.300d.txt', encoding = 'utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "_IhWrPZt5eGZ"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size_dec, 300))\n",
    "for word, i in tokenizer_dec.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(ord)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ki7igi165eGa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "cBqXwC-f5eGa"
   },
   "outputs": [],
   "source": [
    "#saved all these data for further uses\n",
    "\n",
    "with open('tokenizer_enc.pickle', 'wb') as file:\n",
    "    pickle.dump(tokenizer_enc, file)\n",
    "    \n",
    "with open('tokenizer_dec.pickle', 'wb') as file:\n",
    "    pickle.dump(tokenizer_dec, file)\n",
    "    \n",
    "    \n",
    "with open('vocab_size_enc.pickle', 'wb') as file:\n",
    "    pickle.dump(vocab_size_enc, file)\n",
    "    \n",
    "with open('vocab_size_dec.pickle', 'wb') as file:\n",
    "    pickle.dump(vocab_size_dec, file)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0-xCHXq5eGa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "toevKhhQfgED"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZfquV_v5eGa"
   },
   "source": [
    "#### We have divided the  LSTM Model into two parts\n",
    "\n",
    "#### 1. LSTM Without Attention layer\n",
    "\n",
    "#### 2. LSTM With Attention layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUW-DiNLfgED"
   },
   "source": [
    "## 1. LSTM Without Attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "VxY3Ct8l5eGa"
   },
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "lstm_size=128\n",
    "max_dec = 50\n",
    "max_enc = 29\n",
    "embedding_dim = 100\n",
    "dense_units = 256\n",
    "latent_dim=192\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "THBqhCsF5eGa",
    "outputId": "e557513d-160d-49dd-daed-6bc1972c5ccd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 29)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 29, 100)      27212400    ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, None, 100)    3673000     ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(32, 29, 128),      117248      ['embedding[0][0]']              \n",
      "                                 (32, 128),                                                       \n",
      "                                 (32, 128)]                                                       \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(32, None, 128),    117248      ['embedding_1[0][0]',            \n",
      "                                 (32, 128),                       'lstm[0][1]',                   \n",
      "                                 (32, 128)]                       'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " dense (Dense)                  (32, None, 36730)    4738170     ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 35,858,066\n",
      "Trainable params: 35,858,066\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "state=[tf.zeros((batch_size, lstm_size)), tf.zeros((batch_size, lstm_size))]\n",
    "\n",
    "# LSTM Encoder\n",
    "enc_inp = keras.layers.Input(shape=(max_enc,))\n",
    "encoder_embedding = keras.layers.Embedding(input_dim=vocab_size_enc, output_dim=embedding_dim,input_length = max_enc)\n",
    "encoder_lstm = keras.layers.LSTM(lstm_size, return_sequences = True, return_state = True)\n",
    "\n",
    "output_state, enc_h, enc_c = encoder_lstm(encoder_embedding(enc_inp),initial_state=state)\n",
    "\n",
    "# LSTM Decoder\n",
    "dec_inp = keras.layers.Input(shape=(None,))\n",
    "decoder_embedding = keras.layers.Embedding(input_dim=vocab_size_dec, output_dim=embedding_dim,input_length = max_dec)\n",
    "decoder_lstm = keras.layers.LSTM(units=lstm_size , return_sequences = True, return_state = True)\n",
    "\n",
    "output, _ , _ = decoder_lstm(decoder_embedding(dec_inp), initial_state=[enc_h,enc_c])\n",
    "\n",
    "\n",
    "decoder_dense = keras.layers.Dense(vocab_size_dec, activation=\"softmax\")\n",
    "decoder_out = decoder_dense(output)\n",
    "\n",
    "# Define the model that uses the Encoder and the Decoder\n",
    "model_lstm = keras.models.Model([enc_inp, dec_inp], decoder_out)\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "wB0d5sA47Mho",
    "outputId": "1303aa3f-f8d7-41c2-a5a6-33fad537f291"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model_lstm,to_file='model_lstm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0bA-ne4s7raY",
    "outputId": "a293afd3-064b-4588-860e-c0c184b3b14e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1114860\n",
      "drwxr-xr-x 1 root root       4096 Aug  3 20:21 sample_data\n",
      "-rw-r--r-- 1 root root  101363371 Aug 10 16:14 Sequence_data.pickle\n",
      "-rw-r--r-- 1 root root 1037962819 Aug 10 16:15 glove.6B.300d.txt\n",
      "-rw-r--r-- 1 root root    1069446 Aug 10 16:16 tokenizer_enc.pickle\n",
      "-rw-r--r-- 1 root root    1179129 Aug 10 16:16 tokenizer_dec.pickle\n",
      "-rw-r--r-- 1 root root          8 Aug 10 16:16 vocab_size_enc.pickle\n",
      "-rw-r--r-- 1 root root          6 Aug 10 16:16 vocab_size_dec.pickle\n",
      "-rw-r--r-- 1 root root      18712 Aug 10 16:17 model_lstm.png\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "3RmzgUpP7rdv",
    "outputId": "f5928d13-c071-44cd-e7b2-d5afb0612e30"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_2c6e0db4-d05f-484f-af39-a0c7571ea203\", \"model_lstm.png\", 18712)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.download('model_lstm.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLmtlzBZ7rmR"
   },
   "source": [
    "#### We are using the same LSTM Encoder Decoder model in class shape for rest of the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wd8VStBI5eGb"
   },
   "source": [
    "### 1.1 LSTM with Adam Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RASnO12t5eGb"
   },
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uMJHf-FB5eGb"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    \n",
    "\n",
    "    '''\n",
    "    Encoder model -- That takes a input sequence and returns encoder-outputs,encoder_final_state_h,encoder_final_state_c\n",
    "    '''\n",
    "    \n",
    "\n",
    "    def __init__(self,inp_vocab_size,embedding_size,lstm_size,input_length):\n",
    "        \n",
    "\n",
    "        #Initialize Embedding layer\n",
    "        #Intialize Encoder LSTM layer\n",
    "        \n",
    "        super().__init__()\n",
    "        self.lstm_size = lstm_size\n",
    "        #Initialize Embedding layer\n",
    "        self.enc_embed = Embedding(input_dim = inp_vocab_size, output_dim = embedding_size, input_length= input_length)\n",
    "        #Intialize Encoder LSTM layer\n",
    "        self.enc_lstm = LSTM(lstm_size, return_sequences = True, return_state = True)\n",
    "\n",
    "\n",
    "\n",
    "    def call(self,input_sequence,states):\n",
    "        \n",
    "        \n",
    "        '''\n",
    "          This function takes a sequence input and the initial states of the encoder.\n",
    "          Pass the input_sequence input to the Embedding layer, Pass the embedding layer ouput to encoder_lstm\n",
    "          returns -- encoder_output, last time step's hidden and cell state\n",
    "        '''\n",
    "\n",
    "        embedding = self.enc_embed(input_sequence)\n",
    "        output_state, enc_h, enc_c = self.enc_lstm(embedding, initial_state = states)\n",
    "        \n",
    "        return output_state, enc_h, enc_c\n",
    "        \n",
    "\n",
    "    \n",
    "    def initialize_states(self,batch_size):\n",
    "        \n",
    "        '''\n",
    "        Given a batch size it will return intial hidden state and intial cell state.\n",
    "        If batch size is 32- Hidden state is zeros of size [32,lstm_units], cell state zeros is of size [32,lstm_units]\n",
    "        '''\n",
    "        \n",
    "        return [tf.zeros((batch_size, self.lstm_size)), tf.zeros((batch_size, self.lstm_size))]\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCm_B_Iz5eGb"
   },
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JYnDTA3E5eGb"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    '''\n",
    "    Decoder model -- That takes a input sequence and returns output sequence\n",
    "    '''\n",
    "    def __init__(self,out_vocab_size,embedding_size,lstm_size,input_length):\n",
    "        #Initialize Embedding layer\n",
    "        #Intialize Decoder LSTM layer\n",
    "        \n",
    "        super().__init__()\n",
    "        #Initialize Embedding layer\n",
    "        self.dec_embed = Embedding(input_dim = out_vocab_size, output_dim = embedding_size, input_length = input_length)\n",
    "        #Intialize Decoder LSTM layer\n",
    "        self.dec_lstm = LSTM(lstm_size, return_sequences = True, return_state = True)\n",
    "    \n",
    "    def call(self,input_sequence, initial_states):\n",
    "        \n",
    "        '''\n",
    "          This function takes a sequence input and the initial states of the encoder.\n",
    "          Pass the input_sequence input to the Embedding layer, Pass the embedding layer ouput to decoder_lstm\n",
    "        \n",
    "          returns -- decoder_output,decoder_final_state_h,decoder_final_state_c\n",
    "        '''\n",
    "\n",
    "        embedding = self.dec_embed(input_sequence)\n",
    "        \n",
    "        output_state, dec_h, dec_c = self.dec_lstm(embedding, initial_state = initial_states)\n",
    "        \n",
    "        return output_state, dec_h, dec_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIOq1ZN75eGc"
   },
   "source": [
    "#### Encoder_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-16f5XkW5eGc"
   },
   "outputs": [],
   "source": [
    "class Encoder_decoder(tf.keras.Model): \n",
    "    \n",
    "    def __init__(self,*params):\n",
    "    \n",
    "        #Create encoder object\n",
    "        #Create decoder object\n",
    "        #Intialize Dense layer(out_vocab_size) with activation='softmax'\n",
    "    \n",
    "        super().__init__()\n",
    "        #Create encoder object\n",
    "        self.encoder = Encoder(inp_vocab_size = params[0], embedding_size = params[2], lstm_size = params[3], input_length = params[4])\n",
    "        #Create decoder object\n",
    "        self.decoder = Decoder(out_vocab_size = params[1], embedding_size = params[2], lstm_size = params[3], input_length = params[5])\n",
    "        #Intialize Dense layer(out_vocab_size) with activation='softmax'\n",
    "        self.dense = Dense(params[1], activation='softmax')\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, params, training = True):\n",
    "        \n",
    "        '''\n",
    "        1. Pass the input sequence to Encoder layer -- Return encoder_output,encoder_final_state_h,encoder_final_state_c\n",
    "        2. Pass the target sequence to Decoder layer with intial states as encoder_final_state_h,encoder_final_state_C\n",
    "        3. Pass the decoder_outputs into Dense layer \n",
    "        \n",
    "        Return decoder_outputs\n",
    "        '''\n",
    "        \n",
    "\n",
    "        enc_inp, dec_inp = params[0], params[1]\n",
    "        \n",
    "        initial_state = self.encoder.initialize_states(batch_size)\n",
    "        \n",
    "        output_state, enc_h, enc_c = self.encoder(enc_inp, initial_state)\n",
    "        \n",
    "        output, _, _ = self.decoder(dec_inp ,[enc_h, enc_c])\n",
    "        \n",
    "        return self.dense(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CM_EPweU5eGc"
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train, tokenizer_enc, tokenizer_dec, max_enc, max_dec)\n",
    "test_dataset  = Dataset(test, tokenizer_enc, tokenizer_dec, max_enc, max_dec)\n",
    "val_dataset  = Dataset(val, tokenizer_enc, tokenizer_dec, max_enc, max_dec)\n",
    "\n",
    "train_dataloader = Dataloder(train_dataset, batch_size=batch_size)\n",
    "test_dataloader = Dataloder(test_dataset, batch_size=batch_size)\n",
    "val_dataloader = Dataloder(val_dataset, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-TXBoPR5eGd"
   },
   "outputs": [],
   "source": [
    "model = Encoder_decoder(vocab_size_enc, \n",
    "                        vocab_size_dec, \n",
    "                        embedding_dim, \n",
    "                        lstm_size, \n",
    "                        max_enc, \n",
    "                        max_dec, \n",
    "                        dense_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Il4xBr2B5eGd"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'Adam', loss = 'sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ge3MZoHz5eGd"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "log_dir=\"logs/fit/LSTM/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "#A callback is a function passed as an argument to another function. This technique allows a function to call another function.\n",
    "callbacks = [ModelCheckpoint(\"lstm_adam\", save_best_only= True, verbose = 1),\n",
    "             TensorBoard(log_dir = log_dir, histogram_freq=1, write_graph=True),\n",
    "             EarlyStopping(monitor='val_loss',patience = 5, verbose = 1),\n",
    "             ReduceLROnPlateau(monitor='val_loss',patience = 2, verbose = 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VU7kXzcd5eGd",
    "outputId": "edf6b9c4-29bf-43f5-9aa7-bfde72b72d08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "7306/7306 [==============================] - 2602s 356ms/step - loss: 1.4761 - val_loss: 0.9652\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.96517, saving model to lstm_addm\n",
      "Epoch 2/40\n",
      "7306/7306 [==============================] - 2576s 353ms/step - loss: 0.9076 - val_loss: 0.7918\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.96517 to 0.79184, saving model to lstm_addm\n",
      "Epoch 3/40\n",
      "7306/7306 [==============================] - 2736s 375ms/step - loss: 0.7399 - val_loss: 0.6864\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.79184 to 0.68640, saving model to lstm_addm\n",
      "Epoch 4/40\n",
      "7306/7306 [==============================] - 2589s 354ms/step - loss: 0.6328 - val_loss: 0.6166\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.68640 to 0.61656, saving model to lstm_addm\n",
      "Epoch 5/40\n",
      "7306/7306 [==============================] - 2586s 354ms/step - loss: 0.5600 - val_loss: 0.5658\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.61656 to 0.56584, saving model to lstm_addm\n",
      "Epoch 6/40\n",
      "7306/7306 [==============================] - 2573s 352ms/step - loss: 0.5025 - val_loss: 0.5279\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.56584 to 0.52793, saving model to lstm_addm\n",
      "Epoch 7/40\n",
      "7306/7306 [==============================] - 2587s 354ms/step - loss: 0.4609 - val_loss: 0.4994\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.52793 to 0.49937, saving model to lstm_addm\n",
      "Epoch 8/40\n",
      "7306/7306 [==============================] - 2588s 354ms/step - loss: 0.4255 - val_loss: 0.4765\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.49937 to 0.47646, saving model to lstm_addm\n",
      "Epoch 9/40\n",
      "7306/7306 [==============================] - 2611s 357ms/step - loss: 0.3967 - val_loss: 0.4577\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.47646 to 0.45769, saving model to lstm_addm\n",
      "Epoch 10/40\n",
      "7306/7306 [==============================] - 2652s 363ms/step - loss: 0.3718 - val_loss: 0.4427\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.45769 to 0.44271, saving model to lstm_addm\n",
      "Epoch 11/40\n",
      "7306/7306 [==============================] - 2662s 364ms/step - loss: 0.3519 - val_loss: 0.4315\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.44271 to 0.43153, saving model to lstm_addm\n",
      "Epoch 12/40\n",
      "7306/7306 [==============================] - 2647s 362ms/step - loss: 0.3350 - val_loss: 0.4207\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.43153 to 0.42067, saving model to lstm_addm\n",
      "Epoch 13/40\n",
      "7306/7306 [==============================] - 2567s 351ms/step - loss: 0.3196 - val_loss: 0.4128\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.42067 to 0.41277, saving model to lstm_addm\n",
      "Epoch 14/40\n",
      "7306/7306 [==============================] - 2588s 354ms/step - loss: 0.3076 - val_loss: 0.4060\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.41277 to 0.40602, saving model to lstm_addm\n",
      "Epoch 15/40\n",
      "7306/7306 [==============================] - 2601s 356ms/step - loss: 0.2962 - val_loss: 0.4004\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.40602 to 0.40044, saving model to lstm_addm\n",
      "Epoch 16/40\n",
      "7306/7306 [==============================] - 2588s 354ms/step - loss: 0.2849 - val_loss: 0.3962\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.40044 to 0.39621, saving model to lstm_addm\n",
      "Epoch 17/40\n",
      "7306/7306 [==============================] - 2607s 357ms/step - loss: 0.2750 - val_loss: 0.3914\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.39621 to 0.39135, saving model to lstm_addm\n",
      "Epoch 18/40\n",
      "7306/7306 [==============================] - 2589s 354ms/step - loss: 0.2665 - val_loss: 0.3877\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.39135 to 0.38769, saving model to lstm_addm\n",
      "Epoch 19/40\n",
      "7306/7306 [==============================] - 2573s 352ms/step - loss: 0.2592 - val_loss: 0.3862\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.38769 to 0.38623, saving model to lstm_addm\n",
      "Epoch 20/40\n",
      "7306/7306 [==============================] - 2562s 351ms/step - loss: 0.2535 - val_loss: 0.3827\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.38623 to 0.38274, saving model to lstm_addm\n",
      "Epoch 21/40\n",
      "7306/7306 [==============================] - 2587s 354ms/step - loss: 0.2454 - val_loss: 0.3801\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.38274 to 0.38005, saving model to lstm_addm\n",
      "Epoch 22/40\n",
      "7306/7306 [==============================] - 2574s 352ms/step - loss: 0.2404 - val_loss: 0.3789\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.38005 to 0.37895, saving model to lstm_addm\n",
      "Epoch 23/40\n",
      "7306/7306 [==============================] - 2587s 354ms/step - loss: 0.2366 - val_loss: 0.3785\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.37895 to 0.37852, saving model to lstm_addm\n",
      "Epoch 24/40\n",
      "7306/7306 [==============================] - 2586s 354ms/step - loss: 0.2296 - val_loss: 0.3759\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.37852 to 0.37595, saving model to lstm_addm\n",
      "Epoch 25/40\n",
      "7306/7306 [==============================] - 2588s 354ms/step - loss: 0.2255 - val_loss: 0.3752\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.37595 to 0.37516, saving model to lstm_addm\n",
      "Epoch 26/40\n",
      "7306/7306 [==============================] - 2590s 354ms/step - loss: 0.2216 - val_loss: 0.3747\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.37516 to 0.37472, saving model to lstm_addm\n",
      "Epoch 27/40\n",
      "7306/7306 [==============================] - 2605s 357ms/step - loss: 0.2182 - val_loss: 0.3730\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.37472 to 0.37302, saving model to lstm_addm\n",
      "Epoch 28/40\n",
      "7306/7306 [==============================] - 2594s 355ms/step - loss: 0.2149 - val_loss: 0.3735\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.37302\n",
      "Epoch 29/40\n",
      "7306/7306 [==============================] - 2567s 351ms/step - loss: 0.2104 - val_loss: 0.3725\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.37302 to 0.37251, saving model to lstm_addm\n",
      "Epoch 30/40\n",
      "7306/7306 [==============================] - 2626s 359ms/step - loss: 0.2073 - val_loss: 0.3732\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.37251\n",
      "Epoch 31/40\n",
      "7306/7306 [==============================] - 2652s 363ms/step - loss: 0.2034 - val_loss: 0.3716\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.37251 to 0.37157, saving model to lstm_addm\n",
      "Epoch 32/40\n",
      "7306/7306 [==============================] - 2641s 361ms/step - loss: 0.2014 - val_loss: 0.3721\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.37157\n",
      "Epoch 33/40\n",
      "7306/7306 [==============================] - 2635s 361ms/step - loss: 0.1989 - val_loss: 0.3723\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.37157\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 34/40\n",
      "7306/7306 [==============================] - 2612s 358ms/step - loss: 0.1804 - val_loss: 0.3557\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.37157 to 0.35566, saving model to lstm_addm\n",
      "Epoch 35/40\n",
      "7306/7306 [==============================] - 2605s 357ms/step - loss: 0.1672 - val_loss: 0.3544\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.35566 to 0.35441, saving model to lstm_addm\n",
      "Epoch 36/40\n",
      "7306/7306 [==============================] - 2620s 359ms/step - loss: 0.1631 - val_loss: 0.3545\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.35441\n",
      "Epoch 37/40\n",
      "7306/7306 [==============================] - 2643s 362ms/step - loss: 0.1602 - val_loss: 0.3548\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.35441\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 38/40\n",
      "7306/7306 [==============================] - 2605s 357ms/step - loss: 0.1561 - val_loss: 0.3548\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.35441\n",
      "Epoch 39/40\n",
      "7306/7306 [==============================] - 2597s 356ms/step - loss: 0.1559 - val_loss: 0.3549\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.35441\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 40/40\n",
      "7306/7306 [==============================] - 2726s 373ms/step - loss: 0.1553 - val_loss: 0.3549\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.35441\n",
      "Epoch 00040: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2be914a7160>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model training\n",
    "model.fit(x = train_dataloader, \n",
    "          steps_per_epoch = train_dataloader.__len__(),\n",
    "          validation_data = val_dataloader,\n",
    "          validation_steps = val_dataloader.__len__(),\n",
    "          epochs = 40,\n",
    "          verbose = 1,\n",
    "          callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6aepm955eGd",
    "outputId": "5fe1f29c-a2d2-4d1b-edee-c6654c9cc20b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 12268), started 10 days, 21:56:02 ago. (Use '!kill 12268' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-393d71386522ae4d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-393d71386522ae4d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/fit/LSTM/20220726-214001/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m35kGlge5eGd",
    "outputId": "673217da-f39a-49db-d5ad-595cfdc50008"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2435/2435 [==============================] - 165s 68ms/step - loss: 0.3543\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.35427072644233704"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_steps = test_dataloader.__len__()\n",
    "model.evaluate(test_dataloader,steps=test_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EbH6-Omx5eGe"
   },
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YgKDEup25eGe"
   },
   "outputs": [],
   "source": [
    "class pred_Encoder_decoder(tf.keras.Model): \n",
    "    def __init__(self,*params):\n",
    "        super().__init__()\n",
    "        #Create encoder object\n",
    "        self.encoder = Encoder(inp_vocab_size = params[0], embedding_size = params[2], lstm_size = params[3], input_length = params[4])\n",
    "        #Create decoder object\n",
    "        self.decoder = Decoder(out_vocab_size = params[1], embedding_size = params[2], lstm_size = params[3], input_length = params[5])\n",
    "        #Intialize Dense layer(out_vocab_size) with activation='softmax'\n",
    "        self.dense = Dense(params[1], activation='softmax')\n",
    "    \n",
    "    def call(self, params, training = True):\n",
    "        '''\n",
    "        1. Pass the input sequence to Encoder layer -- Return encoder_output,encoder_final_state_h,encoder_final_state_c\n",
    "        2. Pass the target sequence to Decoder layer with intial states as encoder_final_state_h,encoder_final_state_C\n",
    "        3. Pass the decoder_outputs into Dense layer \n",
    "        \n",
    "        Return decoder_outputs\n",
    "        '''\n",
    "        enc_inp = params[0]\n",
    "        initial_state = self.encoder.initialize_states(1)\n",
    "        \n",
    "        output_state, enc_h, enc_c = self.encoder(enc_inp, initial_state)\n",
    "        pred = tf.expand_dims([tokenizer_dec.word_index['<sos>']], 0)\n",
    "        \n",
    "        dec_h = enc_h\n",
    "        dec_c = enc_c\n",
    "        \n",
    "        all_pred = []\n",
    "        \n",
    "        for t in range(max_dec):  \n",
    "            pred, dec_h,dec_c = self.decoder(pred, [dec_h, dec_c])\n",
    "            pred = self.dense(pred)\n",
    "            pred = tf.argmax(pred, axis = -1)\n",
    "            all_pred.append(pred)\n",
    "        \n",
    "        return all_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MfwrvS8h5eGe"
   },
   "outputs": [],
   "source": [
    "pred_model = pred_Encoder_decoder(vocab_size_enc, \n",
    "                                  vocab_size_dec, \n",
    "                                  embedding_dim, \n",
    "                                  lstm_size, \n",
    "                                  max_enc, \n",
    "                                  max_dec, \n",
    "                                  dense_units)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kIBjIEOY5eGe"
   },
   "outputs": [],
   "source": [
    "pred_model.compile(optimizer = 'Adam', loss = 'sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hCrB6RBc5eGe",
    "outputId": "c08ef497-0611-4399-ff69-13c1f3be0d13"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x27158964fa0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the previously trained model\n",
    "pred_model.load_weights('lstm_adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_GO_2215eGe"
   },
   "outputs": [],
   "source": [
    "def predict(input_sequence):\n",
    "    '''\n",
    "    1. Given the input sentence, convert the sentence into integers using the tokenizer used earlier\n",
    "    2. Pass the input_sequence to the encoder. we get encoder_outputs, last time step hidden and cell state\n",
    "    3. Initialize index of <start> as input to decoder. and encoder final states as input_states to the decoder\n",
    "    4. till we reach max_length of decoder or till the model predicted word <end>:\n",
    "         predicted_out,state_h,state_c=model.layers[1](dec_input,states)\n",
    "         pass the predicted_out to the dense layer\n",
    "         update the states=[state_h,state_c]\n",
    "         And get the index of the word with the maximum probability of the dense layer output, using the tokenizer(word index) get the word and then store it in a string.\n",
    "         Update the input_to_decoder with current predictions\n",
    "    5. Return the predicted sentence\n",
    "    '''\n",
    "    seq = input_sequence\n",
    "    seq = '<sos> '+seq+' <eos>'\n",
    "    seq = tokenizer_enc.texts_to_sequences([seq])\n",
    "    \n",
    "    seq = pad_sequences(seq, maxlen=max_dec, padding='post', dtype = np.int32)\n",
    "    pred = pred_model.predict(tf.expand_dims(seq, 0))\n",
    "    output = []\n",
    "    for i in pred:\n",
    "        word = tokenizer_dec.index_word[i[0][0]]\n",
    "        if word == '<eos>':\n",
    "            break\n",
    "        output.append(word)\n",
    "    return ' '.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_WEyY-zB5eGf",
    "outputId": "d966aa5c-d562-47ac-d6db-b666956a12ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  I also\n",
      "predicted output :  need to have to be in the attached credit worksheet with a master firm purchase sale inc under may\n"
     ]
    }
   ],
   "source": [
    "sentence = 'I also'\n",
    "print('input : ', sentence)\n",
    "\n",
    "result = predict(sentence)\n",
    "print('predicted output : ',result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qFEK5TEU5eGf",
    "outputId": "4d4b9e09-6861-45d6-c5a4-5ff6f242d108"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  I also am including a draft of an announcement\n",
      "predicted output :  today\n"
     ]
    }
   ],
   "source": [
    "sentence = 'I also am including a draft of an announcement'\n",
    "print('input : ', sentence)\n",
    "\n",
    "result = predict(sentence)\n",
    "print('predicted output : ',result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70YWBdSI5eGf"
   },
   "source": [
    "### Bleu score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y1rsXpEV5eGf"
   },
   "outputs": [],
   "source": [
    "def BLEUScore():\n",
    "    BLEUscore_list = []\n",
    "    for x in  test['body_enc_seq'].sample(10000):\n",
    "        reference = x.split()\n",
    "        hypothesis = predict(x)\n",
    "        BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis.split())\n",
    "        BLEUscore_list.append(BLEUscore)\n",
    "        \n",
    "    return sum(BLEUscore_list)/len(BLEUscore_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KkMugdau5eGf",
    "outputId": "3291fc50-965b-4c5d-dfe9-03fbda81b1e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4841624183167232\n"
     ]
    }
   ],
   "source": [
    "bleuscore_lstm =BLEUScore()\n",
    "\n",
    "print(bleuscore_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o6pCEhRH5eGf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_am_RDX5eGf"
   },
   "source": [
    "### 1.2. LSTM with RMSProp Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-aplflf5eGg"
   },
   "outputs": [],
   "source": [
    "model2 = Encoder_decoder(vocab_size_enc, \n",
    "                        vocab_size_dec, \n",
    "                        embedding_dim, \n",
    "                        lstm_size, \n",
    "                        max_enc, \n",
    "                        max_dec, \n",
    "                        dense_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FvypWoTX5eGg"
   },
   "outputs": [],
   "source": [
    "model2.compile(optimizer = 'RMSprop', loss = 'sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kXsbzTJu5eGg"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "log_dir=\"logs/fit/LSTM2/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "#A callback is a function passed as an argument to another function. This technique allows a function to call another function.\n",
    "callbacks = [ModelCheckpoint(\"lstm_RMSprop\", save_best_only= True, verbose = 1),\n",
    "             TensorBoard(log_dir = log_dir, histogram_freq=1, write_graph=True),\n",
    "             EarlyStopping(monitor='val_loss',patience = 5, verbose = 1),\n",
    "             ReduceLROnPlateau(monitor='val_loss',patience = 2, verbose = 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "od6m3B0P5eGg",
    "outputId": "83667fb2-27f9-47f4-8cf8-57e479ac1a29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "7306/7306 [==============================] - 2516s 344ms/step - loss: 1.3382 - val_loss: 1.0411\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.04105, saving model to lstm_RMSprop\n",
      "Epoch 2/40\n",
      "7306/7306 [==============================] - 2375s 325ms/step - loss: 1.0179 - val_loss: 0.9891\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.04105 to 0.98906, saving model to lstm_RMSprop\n",
      "Epoch 3/40\n",
      "7306/7306 [==============================] - 2368s 324ms/step - loss: 0.9757 - val_loss: 0.9637\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.98906 to 0.96368, saving model to lstm_RMSprop\n",
      "Epoch 4/40\n",
      "7306/7306 [==============================] - 2359s 323ms/step - loss: 0.9428 - val_loss: 0.9341\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.96368 to 0.93409, saving model to lstm_RMSprop\n",
      "Epoch 5/40\n",
      "7306/7306 [==============================] - 2340s 320ms/step - loss: 0.9062 - val_loss: 0.9010\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.93409 to 0.90101, saving model to lstm_RMSprop\n",
      "Epoch 6/40\n",
      "7306/7306 [==============================] - 2365s 324ms/step - loss: 0.8738 - val_loss: 0.8785\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.90101 to 0.87848, saving model to lstm_RMSprop\n",
      "Epoch 7/40\n",
      "7306/7306 [==============================] - 2432s 333ms/step - loss: 0.8449 - val_loss: 0.8587\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.87848 to 0.85865, saving model to lstm_RMSprop\n",
      "Epoch 8/40\n",
      "7306/7306 [==============================] - 2378s 325ms/step - loss: 0.8263 - val_loss: 0.8403\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.85865 to 0.84031, saving model to lstm_RMSprop\n",
      "Epoch 9/40\n",
      "7306/7306 [==============================] - 2372s 325ms/step - loss: 0.8001 - val_loss: 0.8210\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.84031 to 0.82100, saving model to lstm_RMSprop\n",
      "Epoch 10/40\n",
      "7306/7306 [==============================] - 2357s 323ms/step - loss: 0.7795 - val_loss: 0.8087\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.82100 to 0.80871, saving model to lstm_RMSprop\n",
      "Epoch 11/40\n",
      "7306/7306 [==============================] - 2414s 330ms/step - loss: 0.7628 - val_loss: 0.8004\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.80871 to 0.80042, saving model to lstm_RMSprop\n",
      "Epoch 12/40\n",
      "7306/7306 [==============================] - 2407s 330ms/step - loss: 0.7490 - val_loss: 0.7923\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.80042 to 0.79229, saving model to lstm_RMSprop\n",
      "Epoch 13/40\n",
      "7306/7306 [==============================] - 2437s 334ms/step - loss: 0.7414 - val_loss: 0.7834\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.79229 to 0.78345, saving model to lstm_RMSprop\n",
      "Epoch 14/40\n",
      "7306/7306 [==============================] - 2374s 325ms/step - loss: 0.7255 - val_loss: 0.7741\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.78345 to 0.77410, saving model to lstm_RMSprop\n",
      "Epoch 15/40\n",
      "7306/7306 [==============================] - 2358s 323ms/step - loss: 0.7137 - val_loss: 0.7657\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.77410 to 0.76572, saving model to lstm_RMSprop\n",
      "Epoch 16/40\n",
      "7306/7306 [==============================] - 2358s 323ms/step - loss: 0.7031 - val_loss: 0.7602\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.76572 to 0.76024, saving model to lstm_RMSprop\n",
      "Epoch 17/40\n",
      "7306/7306 [==============================] - 2362s 323ms/step - loss: 0.6959 - val_loss: 0.7502\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.76024 to 0.75018, saving model to lstm_RMSprop\n",
      "Epoch 18/40\n",
      "7306/7306 [==============================] - 2367s 324ms/step - loss: 0.6818 - val_loss: 0.7423\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.75018 to 0.74229, saving model to lstm_RMSprop\n",
      "Epoch 19/40\n",
      "7306/7306 [==============================] - 25663s 4s/step - loss: 0.6726 - val_loss: 0.7375\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.74229 to 0.73746, saving model to lstm_RMSprop\n",
      "Epoch 20/40\n",
      "7306/7306 [==============================] - 2371s 325ms/step - loss: 0.6660 - val_loss: 0.7330\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.73746 to 0.73297, saving model to lstm_RMSprop\n",
      "Epoch 21/40\n",
      "7306/7306 [==============================] - 2379s 326ms/step - loss: 0.6596 - val_loss: 0.7278\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.73297 to 0.72775, saving model to lstm_RMSprop\n",
      "Epoch 22/40\n",
      "7306/7306 [==============================] - 8813s 1s/step - loss: 0.6531 - val_loss: 0.7268\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.72775 to 0.72684, saving model to lstm_RMSprop\n",
      "Epoch 23/40\n",
      "7306/7306 [==============================] - 2436s 333ms/step - loss: 0.6495 - val_loss: 0.7238\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.72684 to 0.72376, saving model to lstm_RMSprop\n",
      "Epoch 24/40\n",
      "7306/7306 [==============================] - 2469s 338ms/step - loss: 0.6439 - val_loss: 0.7195\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.72376 to 0.71950, saving model to lstm_RMSprop\n",
      "Epoch 25/40\n",
      "7306/7306 [==============================] - 2515s 344ms/step - loss: 0.6372 - val_loss: 0.7146\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.71950 to 0.71463, saving model to lstm_RMSprop\n",
      "Epoch 26/40\n",
      "7306/7306 [==============================] - 2378s 326ms/step - loss: 0.6278 - val_loss: 0.7112\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.71463 to 0.71117, saving model to lstm_RMSprop\n",
      "Epoch 27/40\n",
      "7306/7306 [==============================] - 2357s 323ms/step - loss: 0.6233 - val_loss: 0.7064\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.71117 to 0.70643, saving model to lstm_RMSprop\n",
      "Epoch 28/40\n",
      "7306/7306 [==============================] - 2390s 327ms/step - loss: 0.6151 - val_loss: 0.7040\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.70643 to 0.70399, saving model to lstm_RMSprop\n",
      "Epoch 29/40\n",
      "7306/7306 [==============================] - 2394s 328ms/step - loss: 0.6116 - val_loss: 0.7004\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.70399 to 0.70037, saving model to lstm_RMSprop\n",
      "Epoch 30/40\n",
      "7306/7306 [==============================] - 2369s 324ms/step - loss: 0.6045 - val_loss: 0.6969\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.70037 to 0.69695, saving model to lstm_RMSprop\n",
      "Epoch 31/40\n",
      "7306/7306 [==============================] - 2473s 339ms/step - loss: 0.6002 - val_loss: 0.6971\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.69695\n",
      "Epoch 32/40\n",
      "7306/7306 [==============================] - 2495s 342ms/step - loss: 0.5959 - val_loss: 0.6935\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.69695 to 0.69345, saving model to lstm_RMSprop\n",
      "Epoch 33/40\n",
      "7306/7306 [==============================] - 2412s 330ms/step - loss: 0.5911 - val_loss: 0.6911\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.69345 to 0.69107, saving model to lstm_RMSprop\n",
      "Epoch 34/40\n",
      "7306/7306 [==============================] - 2396s 328ms/step - loss: 0.5878 - val_loss: 0.6886\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.69107 to 0.68860, saving model to lstm_RMSprop\n",
      "Epoch 35/40\n",
      "7306/7306 [==============================] - 2370s 324ms/step - loss: 0.5865 - val_loss: 0.6866\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.68860 to 0.68660, saving model to lstm_RMSprop\n",
      "Epoch 36/40\n",
      "7306/7306 [==============================] - 2394s 328ms/step - loss: 0.5784 - val_loss: 0.6840\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.68660 to 0.68401, saving model to lstm_RMSprop\n",
      "Epoch 37/40\n",
      "7306/7306 [==============================] - 2361s 323ms/step - loss: 0.5757 - val_loss: 0.6825\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.68401 to 0.68247, saving model to lstm_RMSprop\n",
      "Epoch 38/40\n",
      "7306/7306 [==============================] - 6353s 870ms/step - loss: 0.5717 - val_loss: 0.6798\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.68247 to 0.67984, saving model to lstm_RMSprop\n",
      "Epoch 39/40\n",
      "7306/7306 [==============================] - 2334s 320ms/step - loss: 0.5700 - val_loss: 0.6778\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.67984 to 0.67775, saving model to lstm_RMSprop\n",
      "Epoch 40/40\n",
      "7306/7306 [==============================] - 2332s 319ms/step - loss: 0.5640 - val_loss: 0.6775\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.67775 to 0.67749, saving model to lstm_RMSprop\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24c83e8c400>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model training\n",
    "model2.fit(x = train_dataloader, \n",
    "          steps_per_epoch = train_dataloader.__len__(),\n",
    "          validation_data = val_dataloader,\n",
    "          validation_steps = val_dataloader.__len__(),\n",
    "          epochs = 40,\n",
    "          verbose = 1,\n",
    "          callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M8WWq73d5eGg",
    "outputId": "b53ab631-cb2f-4c46-a79b-37d1325f5377"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 27900), started 0:01:00 ago. (Use '!kill 27900' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-84292b210a7eed12\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-84292b210a7eed12\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir logs/fit/LSTM2/20220730-205557/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZwlU8PZ25eGg",
    "outputId": "2a0209fc-7880-4dae-e5f5-4cabffab3a43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2435/2435 [==============================] - 232s 95ms/step - loss: 0.6798\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6797993183135986"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_steps = test_dataloader.__len__()\n",
    "model2.evaluate(test_dataloader,steps=test_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXTgKMSW5eGg"
   },
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zIYYSL745eGg"
   },
   "outputs": [],
   "source": [
    "pred_model2 = pred_Encoder_decoder(vocab_size_enc, \n",
    "                                  vocab_size_dec, \n",
    "                                  embedding_dim, \n",
    "                                  lstm_size, \n",
    "                                  max_enc, \n",
    "                                  max_dec, \n",
    "                                  dense_units)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XXxV0HdE5eGh"
   },
   "outputs": [],
   "source": [
    "pred_model2.compile(optimizer = 'RMSprop', loss = 'sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NAPYmqCn5eGh",
    "outputId": "328d0019-442e-46bd-f729-ab7fbd49abc5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x271babc5d90>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the previously trained model\n",
    "pred_model2.load_weights('lstm_RMSprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "us0UHh9i5eGh"
   },
   "outputs": [],
   "source": [
    "def predict(input_sequence):\n",
    "        \n",
    "    '''\n",
    "    1. Given the input sentence, convert the sentence into integers using the tokenizer used earlier\n",
    "    2. Pass the input_sequence to the encoder. we get encoder_outputs, last time step hidden and cell state\n",
    "    3. Initialize index of <start> as input to decoder. and encoder final states as input_states to the decoder\n",
    "    4. till we reach max_length of decoder or till the model predicted word <end>:\n",
    "         predicted_out,state_h,state_c=model.layers[1](dec_input,states)\n",
    "         pass the predicted_out to the dense layer\n",
    "         update the states=[state_h,state_c]\n",
    "         And get the index of the word with the maximum probability of the dense layer output, using the tokenizer(word index) get the word and then store it in a string.\n",
    "         Update the input_to_decoder with current predictions\n",
    "    5. Return the predicted sentence\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    seq = input_sequence\n",
    "    seq = '<sos> '+seq+' <eos>'\n",
    "    seq = tokenizer_enc.texts_to_sequences([seq])\n",
    "    \n",
    "    seq = pad_sequences(seq, maxlen=max_dec, padding='post', dtype = np.int32)\n",
    "    pred = pred_model2.predict(tf.expand_dims(seq, 0))\n",
    "    output = []\n",
    "    for i in pred:\n",
    "        word = tokenizer_dec.index_word[i[0][0]]\n",
    "        if word == '<eos>':\n",
    "            break\n",
    "        output.append(word)\n",
    "    return ' '.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GgXTuQ1N5eGh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OnhQTy2D5eGh",
    "outputId": "ba8b1e4d-01c4-4d25-d9a4-3d77985d3bbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  I also\n",
      "predicted output :  have sure that you want to go to the following the enron has our power trading isda and a same form for that you can be the right hand to get to credit help\n"
     ]
    }
   ],
   "source": [
    "sentence = 'I also'\n",
    "print('input : ', sentence)\n",
    "\n",
    "result = predict(sentence)\n",
    "print('predicted output : ',result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DATgJWok5eGh",
    "outputId": "1b7cad94-b0ef-4873-f6de-ca95735a75e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  I also am including a draft of an announcement\n",
      "predicted output :  of the last month st cc click on the following documents click here for\n"
     ]
    }
   ],
   "source": [
    "sentence = 'I also am including a draft of an announcement'\n",
    "print('input : ', sentence)\n",
    "\n",
    "result = predict(sentence)\n",
    "print('predicted output : ',result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1Dut1aH5eGi"
   },
   "source": [
    "### Bleu score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FW_DDuX85eGi"
   },
   "outputs": [],
   "source": [
    "def BLEUScore():\n",
    "    BLEUscore_list = []\n",
    "    for x in  test['body_enc_seq'].sample(10000):\n",
    "        reference = x.split()\n",
    "        hypothesis = predict(x)\n",
    "        BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis.split())\n",
    "        BLEUscore_list.append(BLEUscore)\n",
    "        \n",
    "    return sum(BLEUscore_list)/len(BLEUscore_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W7kNBHrl5eGi",
    "outputId": "01f566d7-dfdd-4671-b116-565f7169914c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3962702464798132\n"
     ]
    }
   ],
   "source": [
    "bleuscore_lstm2 =BLEUScore()\n",
    "\n",
    "print(bleuscore_lstm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZcMZLje5eGi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eH2RMgH8fgEL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "De3_fqs5fgEL"
   },
   "source": [
    "# 2. LSTM with Attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "bWZWj5PwfgEL"
   },
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "lstm_size=128\n",
    "max_dec = 50\n",
    "max_enc = 29\n",
    "embedding_dim = 100\n",
    "dense_units = 256\n",
    "latent_dim=192\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0-eoTtGxfgEL",
    "outputId": "132476fe-9381-44d5-862f-cb259b433f4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 29)]         0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 29, 100)      27212400    ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, None, 100)    3673000     ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  [(32, 29, 128),      117248      ['embedding_2[0][0]']            \n",
      "                                 (32, 128),                                                       \n",
      "                                 (32, 128)]                                                       \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  [(32, None, 128),    117248      ['embedding_3[0][0]',            \n",
      "                                 (32, 128),                       'lstm_2[0][1]',                 \n",
      "                                 (32, 128)]                       'lstm_2[0][2]']                 \n",
      "                                                                                                  \n",
      " Dot_1 (Dot)                    (32, None, 29)       0           ['lstm_3[0][0]',                 \n",
      "                                                                  'lstm_2[0][0]']                 \n",
      "                                                                                                  \n",
      " attention (Activation)         (32, None, 29)       0           ['Dot_1[0][0]']                  \n",
      "                                                                                                  \n",
      " Dot_2 (Dot)                    (32, None, 128)      0           ['attention[0][0]',              \n",
      "                                                                  'lstm_2[0][0]']                 \n",
      "                                                                                                  \n",
      " Concatenate (Concatenate)      (32, None, 256)      0           ['Dot_2[0][0]',                  \n",
      "                                                                  'lstm_3[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (32, None, 36730)    9439610     ['Concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 40,559,506\n",
      "Trainable params: 40,559,506\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "state=[tf.zeros((batch_size, lstm_size)), tf.zeros((batch_size, lstm_size))]\n",
    "\n",
    "# LSTM Encoder\n",
    "enc_inp = keras.layers.Input(shape=(max_enc,))\n",
    "encoder_embedding = keras.layers.Embedding(input_dim=vocab_size_enc, output_dim=embedding_dim,input_length = max_enc)\n",
    "encoder_lstm = keras.layers.LSTM(lstm_size, return_sequences = True, return_state = True)\n",
    "\n",
    "output_state, enc_h, enc_c = encoder_lstm(encoder_embedding(enc_inp),initial_state=state)\n",
    "\n",
    "# LSTM Decoder\n",
    "dec_inp = keras.layers.Input(shape=(None,))\n",
    "decoder_embedding = keras.layers.Embedding(input_dim=vocab_size_dec, output_dim=embedding_dim,input_length = max_dec)\n",
    "decoder_lstm = keras.layers.LSTM(units=lstm_size , return_sequences = True, return_state = True)\n",
    "\n",
    "output, _ , _ = decoder_lstm(decoder_embedding(dec_inp), initial_state=[enc_h,enc_c])\n",
    "\n",
    "\n",
    "################ define attention  #####################\n",
    "attention = dot([output, output_state], axes=[2, 2],name = 'Dot_1')\n",
    "attention = Activation('softmax', name='attention')(attention)\n",
    "context = dot([attention, output_state], axes=[2,1], name='Dot_2')\n",
    "decoder_combined_context = concatenate([context, output],name='Concatenate')\n",
    "output = Dense(vocab_size_dec, activation=\"softmax\")(decoder_combined_context)\n",
    "final_output = (output)\n",
    "\n",
    "# Define the model that uses the Encoder and the Decoder\n",
    "model_lstm = keras.models.Model([enc_inp, dec_inp], final_output)\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 856
    },
    "id": "9yt5rciCfgEL",
    "outputId": "4755621c-9af9-41fc-d62c-fec8e693e7a9"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model_lstm,to_file='model_lstm_attention.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A8Zm8gcOfgEL",
    "outputId": "6ac8cd7d-7993-4065-92e6-c61c9722d45c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1114900\n",
      "drwxr-xr-x 1 root root       4096 Aug  3 20:21 sample_data\n",
      "-rw-r--r-- 1 root root  101363371 Aug 10 16:14 Sequence_data.pickle\n",
      "-rw-r--r-- 1 root root 1037962819 Aug 10 16:15 glove.6B.300d.txt\n",
      "-rw-r--r-- 1 root root    1069446 Aug 10 16:16 tokenizer_enc.pickle\n",
      "-rw-r--r-- 1 root root    1179129 Aug 10 16:16 tokenizer_dec.pickle\n",
      "-rw-r--r-- 1 root root          8 Aug 10 16:16 vocab_size_enc.pickle\n",
      "-rw-r--r-- 1 root root          6 Aug 10 16:16 vocab_size_dec.pickle\n",
      "-rw-r--r-- 1 root root      18712 Aug 10 16:17 model_lstm.png\n",
      "-rw-r--r-- 1 root root      39658 Aug 10 16:17 model_lstm_attention.png\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "kq_OMbpkfgEM",
    "outputId": "2536cfba-80c0-4192-804d-7b30c7e5c3ee"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_dc2607c6-ac00-44ef-bd48-d6533f253325\", \"model_lstm_attention.png\", 39658)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.download('model_lstm_attention.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmizB31dfgEM"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RB4Qj7WTfgEM"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Encoder model -- That takes a input sequence and returns encoder-outputs,encoder_final_state_h,encoder_final_state_c\n",
    "    '''\n",
    "\n",
    "    def __init__(self,inp_vocab_size,embedding_size,lstm_size,input_length):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm_size = lstm_size\n",
    "        #Initialize Embedding layer\n",
    "        self.enc_embed = Embedding(input_dim = inp_vocab_size, output_dim = embedding_size)\n",
    "        #Intialize Encoder LSTM layer\n",
    "        self.enc_lstm = LSTM(lstm_size, return_sequences = True, return_state = True)\n",
    "        \n",
    "    def call(self,input_sequence,states):\n",
    "        '''\n",
    "          This function takes a sequence input and the initial states of the encoder.\n",
    "          Pass the input_sequence input to the Embedding layer, Pass the embedding layer ouput to encoder_lstm\n",
    "          returns -- encoder_output, last time step's hidden and cell state\n",
    "        '''\n",
    "        embedding = self.enc_embed(input_sequence)\n",
    "        output_state, enc_h, enc_c = self.enc_lstm(embedding, initial_state = states)\n",
    "        return output_state, enc_h, enc_c\n",
    "    \n",
    "    def initialize_states(self,batch_size):\n",
    "      '''\n",
    "      Given a batch size it will return intial hidden state and intial cell state.\n",
    "      If batch size is 32- Hidden state is zeros of size [32,lstm_units], cell state zeros is of size [32,lstm_units]\n",
    "      '''\n",
    "      return [tf.zeros((batch_size, self.lstm_size)), tf.zeros((batch_size, self.lstm_size))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IzD_Vr9fgEM"
   },
   "source": [
    "### Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u7edFkW5fgEM"
   },
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self,scoring_function, att_units):\n",
    "        \n",
    "        # Please go through the reference notebook and research paper to complete the scoring functions\n",
    "        \n",
    "        super(Attention, self).__init__()\n",
    "        self.scoring_function = scoring_function\n",
    "        \n",
    "        # Intialize variables needed for Dot score function here\n",
    "        if scoring_function == 'dot':\n",
    "            self.dot = Dot(axes = (1, 2))\n",
    "            pass\n",
    "        \n",
    "        # Intialize variables needed for General score function here\n",
    "        if scoring_function == 'general':\n",
    "            self.W = Dense(att_units)\n",
    "            self.dot = Dot(axes = (1, 2))\n",
    "            pass\n",
    "        \n",
    "        # Intialize variables needed for Concat score function here\n",
    "        if scoring_function == 'concat':\n",
    "            self.W1 = Dense(att_units)\n",
    "            self.W2 = Dense(att_units)\n",
    "            self.V = Dense(1)\n",
    "            pass\n",
    "        \n",
    "    def call(self,decoder_hidden_state,encoder_output):\n",
    "        \n",
    "        '''\n",
    "        Attention mechanism takes two inputs current step -- decoder_hidden_state and all the encoder_outputs.\n",
    "        Based on the scoring function we will find the score or similarity between decoder_hidden_state and encoder_output.\n",
    "        Multiply the score function with your encoder_outputs to get the context vector.\n",
    "        Function returns context vector and attention weights(softmax - scores)\n",
    "        '''\n",
    "    \n",
    "        decoder_hidden_state = tf.expand_dims(decoder_hidden_state, 1)\n",
    "        \n",
    "        if self.scoring_function == 'dot':\n",
    "            # Implement Dot score function here\n",
    "            score = tf.transpose(self.dot([tf.transpose(decoder_hidden_state, (0, 2, 1)), encoder_output]), (0, 2,1))\n",
    "            pass\n",
    "            \n",
    "        elif self.scoring_function == 'general':\n",
    "            # Implement General score function here\n",
    "            mul = self.W(encoder_output)\n",
    "            score = tf.transpose(self.dot([tf.transpose(decoder_hidden_state, (0, 2, 1)), mul]), (0, 2,1))\n",
    "            pass\n",
    "            \n",
    "        elif self.scoring_function == 'concat':\n",
    "            # Implement General score function here\n",
    "            inter = self.W1(decoder_hidden_state) + self.W2(encoder_output)\n",
    "            tan = tf.nn.tanh(inter)\n",
    "            score = self.V(tan)\n",
    "            pass\n",
    "        \n",
    "        attention_weights = tf.nn.softmax(score, axis =1)\n",
    "        context_vector = attention_weights * encoder_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i6yyZPCCfgEM"
   },
   "source": [
    "### OneStepDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QKOIVbXefgEM"
   },
   "outputs": [],
   "source": [
    "class One_Step_Decoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,tar_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
    "        \n",
    "        # Initialize decoder embedding layer, LSTM and any other objects needed\n",
    "        super().__init__()\n",
    "        self.tar_vocab_size = tar_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.input_dim = input_length\n",
    "        self.lstm_units = dec_units\n",
    "        self.score_fun = score_fun\n",
    "        self.att_units = att_units\n",
    "        self.context_vector = 0\n",
    "        self.attention_weights = 0\n",
    "        self.dec_output = 0\n",
    "        self.decoder_state_h = 0\n",
    "        self.decoder_state_c = 0\n",
    "\n",
    "        self.Embedding_layer = Embedding(input_dim= self.tar_vocab_size, output_dim= self.embedding_dim,input_length= self.input_dim,\n",
    "                                      mask_zero = True, name = \"decoder_embedding_layer\")\n",
    "        self.LSTM_layer = LSTM(units = self.lstm_units, return_sequences= True,return_state= True, name = \"decoder_LSTM_layer\")\n",
    "\n",
    "        self.Attention_layer = Attention(self.score_fun, self.att_units)\n",
    "\n",
    "        self.Dense_layer = Dense(units = self.tar_vocab_size)\n",
    "        \n",
    "    def call(self,input_to_decoder, encoder_output, state_h,state_c):\n",
    "        '''\n",
    "        One step decoder mechanisim step by step:\n",
    "      A. Pass the input_to_decoder to the embedding layer and then get the output(batch_size,1,embedding_dim)\n",
    "      B. Using the encoder_output and decoder hidden state, compute the context vector.\n",
    "      C. Concat the context vector with the step A output\n",
    "      D. Pass the Step-C output to LSTM/GRU and get the decoder output and states(hidden and cell state)\n",
    "      E. Pass the decoder output to dense layer(vocab size) and store the result into output.\n",
    "      F. Return the states from step D, output from Step E, attention weights from Step -B\n",
    "        '''\n",
    "        embedded_output = self.Embedding_layer(input_to_decoder)\n",
    "        \n",
    "        self.context_vector,self.attention_weights = self.Attention_layer(state_h,encoder_output)\n",
    "        self.context_vector = tf.expand_dims(self.context_vector, axis = 1)\n",
    "        \n",
    "        concanated_decoder_input = tf.concat([self.context_vector,embedded_output], axis = -1)\n",
    "        \n",
    "        self.dec_output, self.decoder_state_h, self.decoder_state_c = self.LSTM_layer(concanated_decoder_input,\n",
    "                                                                                      initial_state=[state_h, state_c])\n",
    "        \n",
    "        output = self.Dense_layer(self.dec_output)\n",
    "        output = tf.squeeze(output, axis =1)\n",
    "        \n",
    "        self.context_vector = tf.squeeze(self.context_vector)\n",
    "        \n",
    "        return output, self.decoder_state_h, self.decoder_state_c, self.attention_weights,self.context_vector\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWN8gpd-fgEN"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VafdsNcwfgEN"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,out_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
    "      #Intialize necessary variables and create an object from the class onestepdecoder\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_length = input_length\n",
    "        self.out_vocab_size = out_vocab_size\n",
    "        self.one_step_decoder = One_Step_Decoder(out_vocab_size, \n",
    "                                               embedding_dim, \n",
    "                                               input_length, \n",
    "                                               dec_units,\n",
    "                                               score_fun,\n",
    "                                               att_units)\n",
    "        \n",
    "        self.out_vocab_size = out_vocab_size\n",
    "        \n",
    "    def call(self, input_to_decoder, encoder_output, decoder_hidden_state, decoder_cell_state):\n",
    "        \n",
    "        #Initialize an empty Tensor array, that will store the outputs at each and every time step\n",
    "        #Create a tensor array as shown in the reference notebook\n",
    "        \n",
    "        #Iterate till the length of the decoder input\n",
    "            # Call onestepdecoder for each token in decoder_input\n",
    "            # Store the output in tensorarray\n",
    "        # Return the tensor array\n",
    "        \n",
    "        all_outputs = tf.TensorArray(dtype = tf.float32, size= input_to_decoder.shape[1])\n",
    "        \n",
    "        for timestep in range(input_to_decoder.shape[1]):\n",
    "            output, decoder_hidden_state, decoder_cell_state, _, _ = self.one_step_decoder(input_to_decoder[:, timestep:timestep+1], \n",
    "                                                                                             encoder_output, \n",
    "                                                                                             decoder_hidden_state,\n",
    "                                                                                             decoder_cell_state)\n",
    "            # Store the output in tensorarray\n",
    "            all_outputs = all_outputs.write(timestep, output)\n",
    "        # Return the tensor array\n",
    "        all_outputs = tf.transpose(all_outputs.stack(), (1, 0, 2))\n",
    "        return all_outputs\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tn0wkeDfgEN"
   },
   "source": [
    "### Encoder_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SEymwdqOfgEN"
   },
   "outputs": [],
   "source": [
    "class encoder_decoder(tf.keras.Model):\n",
    "    def __init__(self, inp_vocab_size, out_vocab_size, embedding_dim, enc_units, dec_units, max_ita, max_eng, score_fun, att_units, batch_size):\n",
    "        #Intialize objects from encoder decoder\n",
    "        \n",
    "        super(encoder_decoder, self).__init__()\n",
    "        self.encoder = Encoder(inp_vocab_size, embedding_dim, enc_units, max_ita)\n",
    "        self.one_step_decoder = One_Step_Decoder(out_vocab_size, embedding_dim, max_eng, dec_units ,score_fun ,att_units)\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, data):\n",
    "        \n",
    "        #Intialize encoder states, Pass the encoder_sequence to the embedding layer\n",
    "        # Decoder initial states are encoder final states, Initialize it accordingly\n",
    "        # Pass the decoder sequence,encoder_output,decoder states to Decoder\n",
    "        # return the decoder output\n",
    "\n",
    "        enc_inp, dec_inp = data[0], data[1]\n",
    "        initial_state = self.encoder.initialize_states(self.batch_size)\n",
    "        enc_output, enc_h, enc_c = self.encoder(enc_inp, initial_state)\n",
    "        all_outputs = tf.TensorArray(dtype = tf.float32, size= 50)\n",
    "        \n",
    "        dec_h = enc_h\n",
    "        dec_c = enc_c\n",
    "        for timestep in range(50):\n",
    "            # Call onestepdecoder for each token in decoder_input\n",
    "            output, dec_h, dec_c, _, _ = self.one_step_decoder(dec_inp[:, timestep:timestep+1], \n",
    "                                                               enc_output, \n",
    "                                                               dec_h,\n",
    "                                                               dec_c)\n",
    "            # Store the output in tensorarray\n",
    "            all_outputs = all_outputs.write(timestep, output)\n",
    "        # Return the tensor array\n",
    "        all_outputs = tf.transpose(all_outputs.stack(), (1, 0, 2))\n",
    "        # return the decoder output\n",
    "        return all_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzrxsvi6fgEN"
   },
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gzlJKd39fgEO"
   },
   "outputs": [],
   "source": [
    "def custom_lossfunction(targets,logits):\n",
    "    \n",
    "    # Custom loss function that will not consider the loss for padded zeros.\n",
    "    # Refer https://www.tensorflow.org/tutorials/text/nmt_with_attention#define_the_optimizer_and_the_loss_function\n",
    "    \n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    \n",
    "    mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
    "    \n",
    "    loss_ = loss_object(targets, logits)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    \n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CLj56HGfgEO"
   },
   "source": [
    "# 2.1 Attention with Dot score function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQdNJOR-fgEO"
   },
   "outputs": [],
   "source": [
    "# Implement teacher forcing while training your model. You can do it two ways.\n",
    "# Prepare your data, encoder_input,decoder_input and decoder_output\n",
    "# if decoder input is \n",
    "# <start> Hi how are you\n",
    "# decoder output should be\n",
    "# Hi How are you <end>\n",
    "# i.e when you have send <start>-- decoder predicted Hi, 'Hi' decoder predicted 'How' .. e.t.c\n",
    "\n",
    "# or\n",
    " \n",
    "# model.fit([train_ita,train_eng],train_eng[:,1:]..)\n",
    "# Note: If you follow this approach some grader functions might return false and this is fine.\n",
    "\n",
    "batch_size=32\n",
    "lstm_size=128\n",
    "max_dec = 50\n",
    "max_enc = 29\n",
    "embedding_dim = 100\n",
    "dense_units = 256\n",
    "att_units = 256\n",
    "latent_dim=192\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DXLi0RX2fgEO"
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train, tokenizer_enc, tokenizer_dec, max_enc, max_dec)\n",
    "test_dataset  = Dataset(test, tokenizer_enc, tokenizer_dec, max_enc, max_dec)\n",
    "val_dataset  = Dataset(val, tokenizer_enc, tokenizer_dec, max_enc, max_dec)\n",
    "\n",
    "train_dataloader = Dataloder(train_dataset, batch_size=batch_size)\n",
    "test_dataloader = Dataloder(test_dataset, batch_size=batch_size)\n",
    "val_dataloader = Dataloder(val_dataset, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8JCdk2QlfgEO"
   },
   "outputs": [],
   "source": [
    "model = encoder_decoder(vocab_size_enc, \n",
    "                        vocab_size_dec, \n",
    "                        embedding_dim, \n",
    "                        lstm_size,\n",
    "                        lstm_size,\n",
    "                        max_enc, \n",
    "                        max_dec,\n",
    "                        'dot', \n",
    "                        att_units, \n",
    "                        batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kwYp5CRYfgEO"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'Adam', loss = custom_lossfunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHwtdxGkfgEO"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "log_dir=\"logs/fit/dot/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "#A callback is a function passed as an argument to another function. This technique allows a function to call another function.\n",
    "callbacks = [ModelCheckpoint('dot', save_best_only= True, verbose = 1),\n",
    "             TensorBoard(log_dir = log_dir, histogram_freq=1, write_graph=True),\n",
    "             EarlyStopping(patience = 5, verbose = 1),\n",
    "             ReduceLROnPlateau(patience = 2, verbose = 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9I1swYP7fgEP"
   },
   "outputs": [],
   "source": [
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04Sxv_kdfgEP"
   },
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lcar36aCfgEP",
    "outputId": "edf6b9c4-29bf-43f5-9aa7-bfde72b72d08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 475s 433ms/step - loss: 1.4018 - val_loss: 1.2550\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.25500, saving model to dot\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 392s 392ms/step - loss: 1.2020 - val_loss: 1.1830\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.25500 to 1.18301, saving model to dot\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 399s 399ms/step - loss: 1.1348 - val_loss: 1.1210\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.18301 to 1.12100, saving model to dot\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 394s 394ms/step - loss: 1.0727 - val_loss: 1.0737\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.12100 to 1.07369, saving model to dot\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 396s 396ms/step - loss: 1.0185 - val_loss: 1.0306\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.07369 to 1.03063, saving model to dot\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 394s 394ms/step - loss: 0.9816 - val_loss: 0.9928\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.03063 to 0.99284, saving model to dot\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 403s 403ms/step - loss: 0.9446 - val_loss: 0.9613\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.99284 to 0.96128, saving model to dot\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 413s 413ms/step - loss: 0.9065 - val_loss: 0.9306\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.96128 to 0.93059, saving model to dot\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 403s 403ms/step - loss: 0.8740 - val_loss: 0.9041\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.93059 to 0.90411, saving model to dot\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 404s 404ms/step - loss: 0.8438 - val_loss: 0.8807\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.90411 to 0.88074, saving model to dot\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 403s 403ms/step - loss: 0.8283 - val_loss: 0.8566\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.88074 to 0.85663, saving model to dot\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 397s 397ms/step - loss: 0.8013 - val_loss: 0.8358\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.85663 to 0.83580, saving model to dot\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 415s 415ms/step - loss: 0.7681 - val_loss: 0.8155\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.83580 to 0.81549, saving model to dot\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 412s 412ms/step - loss: 0.7537 - val_loss: 0.7958\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.81549 to 0.79583, saving model to dot\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 401s 401ms/step - loss: 0.7309 - val_loss: 0.7781\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.79583 to 0.77805, saving model to dot\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 403s 403ms/step - loss: 0.7150 - val_loss: 0.7591\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.77805 to 0.75906, saving model to dot\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 404s 404ms/step - loss: 0.6918 - val_loss: 0.7435\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.75906 to 0.74346, saving model to dot\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 403s 403ms/step - loss: 0.6728 - val_loss: 0.7261\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.74346 to 0.72607, saving model to dot\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 419s 419ms/step - loss: 0.6556 - val_loss: 0.7107\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.72607 to 0.71068, saving model to dot\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 422s 422ms/step - loss: 0.6426 - val_loss: 0.6975\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.71068 to 0.69750, saving model to dot\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 391s 391ms/step - loss: 0.6242 - val_loss: 0.6835\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.69750 to 0.68347, saving model to dot\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 400s 400ms/step - loss: 0.6085 - val_loss: 0.6711\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.68347 to 0.67106, saving model to dot\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 402s 402ms/step - loss: 0.5912 - val_loss: 0.6581\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.67106 to 0.65808, saving model to dot\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 407s 407ms/step - loss: 0.5854 - val_loss: 0.6439\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.65808 to 0.64386, saving model to dot\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 409s 409ms/step - loss: 0.5712 - val_loss: 0.6325\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.64386 to 0.63248, saving model to dot\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 400s 400ms/step - loss: 0.5537 - val_loss: 0.6234\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.63248 to 0.62336, saving model to dot\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 399s 399ms/step - loss: 0.5386 - val_loss: 0.6121\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.62336 to 0.61207, saving model to dot\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 401s 401ms/step - loss: 0.5327 - val_loss: 0.6018\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.61207 to 0.60184, saving model to dot\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 423s 423ms/step - loss: 0.5217 - val_loss: 0.5923\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.60184 to 0.59233, saving model to dot\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 390s 390ms/step - loss: 0.5037 - val_loss: 0.5825\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.59233 to 0.58250, saving model to dot\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 402s 402ms/step - loss: 0.4959 - val_loss: 0.5737\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.58250 to 0.57370, saving model to dot\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 406s 406ms/step - loss: 0.4837 - val_loss: 0.5650\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.57370 to 0.56503, saving model to dot\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 402s 402ms/step - loss: 0.4777 - val_loss: 0.5584\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.56503 to 0.55839, saving model to dot\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 405s 405ms/step - loss: 0.4731 - val_loss: 0.5502\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.55839 to 0.55023, saving model to dot\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 416s 416ms/step - loss: 0.4623 - val_loss: 0.5401\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.55023 to 0.54006, saving model to dot\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 396s 396ms/step - loss: 0.4452 - val_loss: 0.5345\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.54006 to 0.53453, saving model to dot\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 401s 401ms/step - loss: 0.4478 - val_loss: 0.5284\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.53453 to 0.52839, saving model to dot\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 400s 400ms/step - loss: 0.4358 - val_loss: 0.5211\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.52839 to 0.52110, saving model to dot\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 399s 399ms/step - loss: 0.4307 - val_loss: 0.5154\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.52110 to 0.51541, saving model to dot\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 411s 411ms/step - loss: 0.4180 - val_loss: 0.5097\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.51541 to 0.50973, saving model to dot\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 406s 407ms/step - loss: 0.4138 - val_loss: 0.5034\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.50973 to 0.50341, saving model to dot\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 385s 385ms/step - loss: 0.4098 - val_loss: 0.4966\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.50341 to 0.49660, saving model to dot\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 395s 395ms/step - loss: 0.3978 - val_loss: 0.4924\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.49660 to 0.49237, saving model to dot\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 396s 396ms/step - loss: 0.3910 - val_loss: 0.4880\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.49237 to 0.48795, saving model to dot\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 417s 417ms/step - loss: 0.3875 - val_loss: 0.4810\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.48795 to 0.48105, saving model to dot\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 416s 416ms/step - loss: 0.3772 - val_loss: 0.4743\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.48105 to 0.47426, saving model to dot\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 384s 384ms/step - loss: 0.3734 - val_loss: 0.4707\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.47426 to 0.47074, saving model to dot\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 394s 394ms/step - loss: 0.3701 - val_loss: 0.4651\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.47074 to 0.46510, saving model to dot\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 397s 397ms/step - loss: 0.3648 - val_loss: 0.4623\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.46510 to 0.46231, saving model to dot\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 405s 405ms/step - loss: 0.3583 - val_loss: 0.4604\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.46231 to 0.46039, saving model to dot\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21727d2d340>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model training\n",
    "model.fit(x = train_dataloader, \n",
    "          steps_per_epoch = 1000, #train_dataloader.__len__(),\n",
    "          validation_data = val_dataloader,\n",
    "          validation_steps = 100, #val_dataloader.__len__(),\n",
    "          epochs = 50,\n",
    "          verbose = 1,\n",
    "          callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nvnrb4glfgEP",
    "outputId": "5fe1f29c-a2d2-4d1b-edee-c6654c9cc20b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 11780), started 0:01:03 ago. (Use '!kill 11780' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-2e70238606983344\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-2e70238606983344\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir logs/fit/dot/20220808-030237/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w8SqRJolfgEP",
    "outputId": "673217da-f39a-49db-d5ad-595cfdc50008"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 48s 72ms/step - loss: 0.4381 0s - loss: 0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.43665775656700134"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_steps = 400 #test_dataloader.__len__()\n",
    "model.evaluate(test_dataloader,steps=test_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKGkiQQ_fgEP"
   },
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kBzy3UdEfgEP"
   },
   "outputs": [],
   "source": [
    "class pred_Encoder_decoder(tf.keras.Model): \n",
    "    def __init__(self, inp_vocab_size, out_vocab_size, embedding_dim, enc_units, dec_units, max_ita, max_eng, score_fun, att_units):\n",
    "        #Intialize objects from encoder decoder\n",
    "        super(pred_Encoder_decoder, self).__init__()\n",
    "        self.encoder = Encoder(inp_vocab_size, embedding_dim, enc_units, max_ita)\n",
    "        self.one_step_decoder = One_Step_Decoder(out_vocab_size, embedding_dim, max_eng, dec_units ,score_fun ,att_units)\n",
    "        self.batch_size = batch_size\n",
    "    def call(self, params):\n",
    "        enc_inp = params[0]\n",
    "        initial_state = self.encoder.initialize_states(1)\n",
    "        output_state, enc_h, enc_c = self.encoder(enc_inp, initial_state)\n",
    "        pred = tf.expand_dims([tokenizer_dec.word_index['<sos>']], 0)\n",
    "        dec_h = enc_h\n",
    "        dec_c = enc_c\n",
    "        all_pred = []\n",
    "        all_attention = []\n",
    "        for t in range(50):  \n",
    "            pred, dec_h,dec_c, attention, _ = self.one_step_decoder(pred, output_state, dec_h, dec_c)\n",
    "            pred = tf.argmax(pred, axis = -1)\n",
    "            all_pred.append(pred)\n",
    "            pred = tf.expand_dims(pred, 0)\n",
    "            all_attention.append(attention)\n",
    "        return all_pred, all_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WKhCPZclfgEQ"
   },
   "outputs": [],
   "source": [
    "pred_model = pred_Encoder_decoder(vocab_size_enc, \n",
    "                                  vocab_size_dec, \n",
    "                                  embedding_dim, \n",
    "                                  lstm_size,\n",
    "                                  lstm_size,\n",
    "                                  max_enc, \n",
    "                                  max_dec, \n",
    "                                  'dot',\n",
    "                                  att_units)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iiitrVEmfgEQ"
   },
   "outputs": [],
   "source": [
    "pred_model.compile(optimizer = 'Adam', loss = custom_lossfunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekAPlTx3fgEQ",
    "outputId": "c08ef497-0611-4399-ff69-13c1f3be0d13"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x2190396fd00>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the previously trained model\n",
    "pred_model.load_weights('dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tUeaweYlfgEQ"
   },
   "outputs": [],
   "source": [
    "def predict(input_sequence):\n",
    "    '''\n",
    "    A. Given input sentence, convert the sentence into integers using tokenizer used earlier\n",
    "    B. Pass the input_sequence to encoder. we get encoder_outputs, last time step hidden and cell state\n",
    "    C. Initialize index of <start> as input to decoder. and encoder final states as input_states to onestepdecoder.\n",
    "    D. till we reach max_length of decoder or till the model predicted word <end>:\n",
    "         predictions, input_states, attention_weights = model.layers[1].onestepdecoder(input_to_decoder, encoder_output, input_states)\n",
    "         Save the attention weights\n",
    "         And get the word using the tokenizer(word index) and then store it in a string.\n",
    "    E. Call plot_attention(#params)\n",
    "    F. Return the predicted sentence\n",
    "    '''\n",
    "    \n",
    "    seq = input_sequence\n",
    "    seq = '<sos> '+seq+' <eos>'\n",
    "    seq = tokenizer_enc.texts_to_sequences([seq])\n",
    "    seq = pad_sequences(seq, maxlen=max_enc, padding='post', dtype = np.int32)\n",
    "    pred, _ = pred_model.predict(tf.expand_dims(seq, 0))\n",
    "    output = []\n",
    "    for i in pred:\n",
    "        word = tokenizer_dec.index_word[i[0]]\n",
    "        if word == '<eos>':\n",
    "            break\n",
    "        output.append(word)\n",
    "    return ' '.join(output)  #, np.squeeze(np.squeeze(np.array(attention_weights), 1), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I4aN0XXCfgEQ",
    "outputId": "d966aa5c-d562-47ac-d6db-b666956a12ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  Your support of\n",
      "predicted output :  the property of enron corp\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Your support of'\n",
    "print('input : ', sentence)\n",
    "\n",
    "result = predict(sentence)\n",
    "\n",
    "print('predicted output : ',result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ksD-38J3fgEQ",
    "outputId": "4d4b9e09-6861-45d6-c5a4-5ff6f242d108"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  I also am including a draft of an announcement\n",
      "predicted output :  and my family is\n"
     ]
    }
   ],
   "source": [
    "sentence = 'I also am including a draft of an announcement'\n",
    "print('input : ', sentence)\n",
    "\n",
    "result = predict(sentence)\n",
    "print('predicted output : ',result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gnlV5Wu-fgEQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVo1avfhfgEQ"
   },
   "source": [
    "### Bleu score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w8oxYIYOfgER"
   },
   "outputs": [],
   "source": [
    "def BLEUScore():\n",
    "    BLEUscore_list = []\n",
    "    \n",
    "    for x in  test['body_enc_seq'].sample(10000):\n",
    "        reference = x.split()\n",
    "        hypothesis = predict(x)\n",
    "        BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis.split())\n",
    "        BLEUscore_list.append(BLEUscore)\n",
    "        \n",
    "    return sum(BLEUscore_list)/len(BLEUscore_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cHiOmez6fgER",
    "outputId": "3291fc50-965b-4c5d-dfe9-03fbda81b1e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4919830571604262\n"
     ]
    }
   ],
   "source": [
    "bleuscore_lstm =BLEUScore()\n",
    "\n",
    "print(bleuscore_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtkbSHOSfgER"
   },
   "source": [
    "# 2.2 Attention with General score function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJ4eYMgCfgER"
   },
   "outputs": [],
   "source": [
    "#Compile and train your model on general scoring function.\n",
    "# Visualize few sentences randomly in Test data\n",
    "# Predict on 1000 random sentences on test data and calculate the average BLEU score of these sentences.\n",
    "# https://www.nltk.org/_modules/nltk/translate/bleu_score.html\n",
    "\n",
    "\n",
    "batch_size=16\n",
    "lstm_size=16\n",
    "att_units = 16\n",
    "max_dec = 50\n",
    "max_enc = 29\n",
    "embedding_dim = 100\n",
    "dense_units = 256\n",
    "latent_dim=192\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xTv95ebXfgER"
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train, tokenizer_enc, tokenizer_dec, max_enc, max_dec)\n",
    "test_dataset  = Dataset(test, tokenizer_enc, tokenizer_dec, max_enc, max_dec)\n",
    "val_dataset  = Dataset(val, tokenizer_enc, tokenizer_dec, max_enc, max_dec)\n",
    "\n",
    "train_dataloader = Dataloder(train_dataset, batch_size=batch_size)\n",
    "test_dataloader = Dataloder(test_dataset, batch_size=batch_size)\n",
    "val_dataloader = Dataloder(val_dataset, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7KaxZrAEfgER"
   },
   "outputs": [],
   "source": [
    "model = encoder_decoder(vocab_size_enc, \n",
    "                        vocab_size_dec, \n",
    "                        embedding_dim, \n",
    "                        lstm_size,\n",
    "                        lstm_size,\n",
    "                        max_enc, \n",
    "                        max_dec,\n",
    "                        'general', \n",
    "                        att_units, \n",
    "                        batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LZ_Kcgf9fgER"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'Adam', loss = custom_lossfunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YzPZTUYAfgER"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "log_dir=\"logs/fit/general/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "#A callback is a function passed as an argument to another function. This technique allows a function to call another function.\n",
    "callbacks = [ModelCheckpoint('general', save_best_only= True, verbose = 1),\n",
    "             TensorBoard(log_dir = log_dir, histogram_freq=1, write_graph=True),\n",
    "             EarlyStopping(monitor='val_loss', patience = 5, verbose = 1),\n",
    "             ReduceLROnPlateau(monitor='val_loss', patience = 2, verbose = 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8T1ctD5GfgES"
   },
   "outputs": [],
   "source": [
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zOdDP_jfgES"
   },
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZY65FZeKfgES",
    "outputId": "edf6b9c4-29bf-43f5-9aa7-bfde72b72d08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 405s 360ms/step - loss: 1.5644 - val_loss: 1.3381\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.33812, saving model to general\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 326s 326ms/step - loss: 1.2882 - val_loss: 1.3120\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.33812 to 1.31201, saving model to general\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 310s 310ms/step - loss: 1.2626 - val_loss: 1.2904\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.31201 to 1.29037, saving model to general\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 335s 335ms/step - loss: 1.2401 - val_loss: 1.2675\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.29037 to 1.26754, saving model to general\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 329s 329ms/step - loss: 1.2269 - val_loss: 1.2442\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.26754 to 1.24415, saving model to general\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 330s 330ms/step - loss: 1.1966 - val_loss: 1.2247\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.24415 to 1.22468, saving model to general\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 340s 340ms/step - loss: 1.1622 - val_loss: 1.2071\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.22468 to 1.20709, saving model to general\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 338s 338ms/step - loss: 1.1414 - val_loss: 1.1913\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.20709 to 1.19128, saving model to general\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 331s 331ms/step - loss: 1.1477 - val_loss: 1.1752\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.19128 to 1.17521, saving model to general\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 351s 351ms/step - loss: 1.1171 - val_loss: 1.1615\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.17521 to 1.16146, saving model to general\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 340s 340ms/step - loss: 1.0978 - val_loss: 1.1484\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.16146 to 1.14836, saving model to general\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 346s 346ms/step - loss: 1.0854 - val_loss: 1.1366\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.14836 to 1.13655, saving model to general\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 346s 346ms/step - loss: 1.0847 - val_loss: 1.1272\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.13655 to 1.12724, saving model to general\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 339s 339ms/step - loss: 1.0686 - val_loss: 1.1171\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.12724 to 1.11712, saving model to general\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 341s 341ms/step - loss: 1.0707 - val_loss: 1.1082\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.11712 to 1.10819, saving model to general\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 347s 347ms/step - loss: 1.0467 - val_loss: 1.0990\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.10819 to 1.09897, saving model to general\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 347s 347ms/step - loss: 1.0468 - val_loss: 1.0916\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.09897 to 1.09158, saving model to general\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 348s 348ms/step - loss: 1.0311 - val_loss: 1.0853\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.09158 to 1.08531, saving model to general\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 342s 342ms/step - loss: 1.0241 - val_loss: 1.0782\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.08531 to 1.07825, saving model to general\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 350s 350ms/step - loss: 1.0180 - val_loss: 1.0725\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.07825 to 1.07249, saving model to general\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 343s 343ms/step - loss: 1.0307 - val_loss: 1.0657\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.07249 to 1.06566, saving model to general\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 349s 349ms/step - loss: 0.9905 - val_loss: 1.0590\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.06566 to 1.05904, saving model to general\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 348s 348ms/step - loss: 0.9924 - val_loss: 1.0538\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.05904 to 1.05380, saving model to general\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 345s 345ms/step - loss: 0.9857 - val_loss: 1.0486\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.05380 to 1.04855, saving model to general\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 354s 354ms/step - loss: 0.9986 - val_loss: 1.0436\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.04855 to 1.04364, saving model to general\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 347s 347ms/step - loss: 0.9869 - val_loss: 1.0393\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.04364 to 1.03927, saving model to general\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 354s 354ms/step - loss: 0.9796 - val_loss: 1.0344\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.03927 to 1.03438, saving model to general\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 349s 349ms/step - loss: 0.9720 - val_loss: 1.0302\n",
      "\n",
      "Epoch 00028: val_loss improved from 1.03438 to 1.03021, saving model to general\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 352s 352ms/step - loss: 0.9700 - val_loss: 1.0262\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.03021 to 1.02618, saving model to general\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 367s 367ms/step - loss: 0.9664 - val_loss: 1.0221\n",
      "\n",
      "Epoch 00030: val_loss improved from 1.02618 to 1.02207, saving model to general\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 365s 365ms/step - loss: 0.9652 - val_loss: 1.0177\n",
      "\n",
      "Epoch 00031: val_loss improved from 1.02207 to 1.01774, saving model to general\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 365s 365ms/step - loss: 0.9547 - val_loss: 1.0134\n",
      "\n",
      "Epoch 00032: val_loss improved from 1.01774 to 1.01341, saving model to general\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 359s 359ms/step - loss: 0.9436 - val_loss: 1.0088\n",
      "\n",
      "Epoch 00033: val_loss improved from 1.01341 to 1.00876, saving model to general\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 358s 358ms/step - loss: 0.9534 - val_loss: 1.0064\n",
      "\n",
      "Epoch 00034: val_loss improved from 1.00876 to 1.00644, saving model to general\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 360s 360ms/step - loss: 0.9470 - val_loss: 1.0016\n",
      "\n",
      "Epoch 00035: val_loss improved from 1.00644 to 1.00165, saving model to general\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 360s 360ms/step - loss: 0.9240 - val_loss: 0.9965\n",
      "\n",
      "Epoch 00036: val_loss improved from 1.00165 to 0.99654, saving model to general\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 360s 360ms/step - loss: 0.9318 - val_loss: 0.9928\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.99654 to 0.99284, saving model to general\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 359s 359ms/step - loss: 0.9234 - val_loss: 0.9912\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.99284 to 0.99116, saving model to general\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 358s 358ms/step - loss: 0.9297 - val_loss: 0.9876\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.99116 to 0.98762, saving model to general\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 359s 359ms/step - loss: 0.9264 - val_loss: 0.9857\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.98762 to 0.98567, saving model to general\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 359s 359ms/step - loss: 0.9273 - val_loss: 0.9813\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.98567 to 0.98130, saving model to general\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 360s 360ms/step - loss: 0.9117 - val_loss: 0.9782\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.98130 to 0.97823, saving model to general\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 360s 360ms/step - loss: 0.9154 - val_loss: 0.9746\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.97823 to 0.97463, saving model to general\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 360s 360ms/step - loss: 0.9087 - val_loss: 0.9724\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.97463 to 0.97241, saving model to general\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 360s 360ms/step - loss: 0.9076 - val_loss: 0.9695\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.97241 to 0.96952, saving model to general\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 362s 362ms/step - loss: 0.9055 - val_loss: 0.9668\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.96952 to 0.96677, saving model to general\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 357s 357ms/step - loss: 0.8935 - val_loss: 0.9639\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.96677 to 0.96386, saving model to general\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 359s 359ms/step - loss: 0.8877 - val_loss: 0.9609\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.96386 to 0.96092, saving model to general\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 363s 363ms/step - loss: 0.8957 - val_loss: 0.9586\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.96092 to 0.95856, saving model to general\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 357s 357ms/step - loss: 0.8885 - val_loss: 0.9564\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.95856 to 0.95641, saving model to general\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2a309676fd0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model training\n",
    "model.fit(x = train_dataloader, \n",
    "          steps_per_epoch = 1000, #train_dataloader.__len__(),\n",
    "          validation_data = val_dataloader,\n",
    "          validation_steps = 100, #val_dataloader.__len__(),\n",
    "          epochs = 50,\n",
    "          verbose = 1,\n",
    "          callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UvH__XRnfgES",
    "outputId": "dfac330c-c0aa-4ddc-e709-18aeaa64ba97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 4128), started 16:50:31 ago. (Use '!kill 4128' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-29c6732105dc5664\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-29c6732105dc5664\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir logs/fit/general/20220808-164739/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u2vAFkoZfgES",
    "outputId": "b4c9837f-b572-4463-c068-3f2aad1d259b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1f7b2242fa0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('general')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dNLYgR0sfgES",
    "outputId": "71a8c6d8-1153-4d6e-ccc0-2484183ee951"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).one_step_decoder.Embedding_layer.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).one_step_decoder.Dense_layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).one_step_decoder.Dense_layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).encoder.enc_lstm.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).encoder.enc_lstm.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).encoder.enc_lstm.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).one_step_decoder.LSTM_layer.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).one_step_decoder.LSTM_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).one_step_decoder.LSTM_layer.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).one_step_decoder.Attention_layer.W.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).one_step_decoder.Attention_layer.W.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).encoder.enc_embed.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).one_step_decoder.Embedding_layer.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).one_step_decoder.Dense_layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).one_step_decoder.Dense_layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).encoder.enc_lstm.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).encoder.enc_lstm.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).encoder.enc_lstm.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).one_step_decoder.LSTM_layer.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).one_step_decoder.LSTM_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).one_step_decoder.LSTM_layer.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).one_step_decoder.Attention_layer.W.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).one_step_decoder.Attention_layer.W.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).encoder.enc_embed.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).one_step_decoder.Embedding_layer.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).one_step_decoder.Dense_layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).one_step_decoder.Dense_layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).encoder.enc_lstm.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).encoder.enc_lstm.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).encoder.enc_lstm.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).one_step_decoder.LSTM_layer.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).one_step_decoder.LSTM_layer.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).one_step_decoder.LSTM_layer.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).one_step_decoder.Attention_layer.W.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).one_step_decoder.Attention_layer.W.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "400/400 [==============================] - 43s 56ms/step - loss: 0.9184\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9128780961036682"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_steps = 400 #test_dataloader.__len__()\n",
    "model.evaluate(test_dataloader,steps=test_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gwcVUp7BfgES"
   },
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jycQy9KqfgET"
   },
   "outputs": [],
   "source": [
    "class pred_Encoder_decoder(tf.keras.Model): \n",
    "    def __init__(self, inp_vocab_size, out_vocab_size, embedding_dim, enc_units, dec_units, max_ita, max_eng, score_fun, att_units):\n",
    "        #Intialize objects from encoder decoder\n",
    "        super(pred_Encoder_decoder, self).__init__()\n",
    "        self.encoder = Encoder(inp_vocab_size, embedding_dim, enc_units, max_ita)\n",
    "        self.one_step_decoder = One_Step_Decoder(out_vocab_size, embedding_dim, max_eng, dec_units ,score_fun ,att_units)\n",
    "        self.batch_size = batch_size\n",
    "    def call(self, params):\n",
    "        enc_inp = params[0]\n",
    "        initial_state = self.encoder.initialize_states(1)\n",
    "        output_state, enc_h, enc_c = self.encoder(enc_inp, initial_state)\n",
    "        pred = tf.expand_dims([tokenizer_dec.word_index['<sos>']], 0)\n",
    "        dec_h = enc_h\n",
    "        dec_c = enc_c\n",
    "        all_pred = []\n",
    "        all_attention = []\n",
    "        for t in range(50):  \n",
    "            pred, dec_h,dec_c, attention, _ = self.one_step_decoder(pred, output_state, dec_h, dec_c)\n",
    "            pred = tf.argmax(pred, axis = -1)\n",
    "            all_pred.append(pred)\n",
    "            pred = tf.expand_dims(pred, 0)\n",
    "            all_attention.append(attention)\n",
    "        return all_pred, all_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QS_yXDFpfgET"
   },
   "outputs": [],
   "source": [
    "pred_model = pred_Encoder_decoder(vocab_size_enc, \n",
    "                                  vocab_size_dec, \n",
    "                                  embedding_dim, \n",
    "                                  lstm_size,\n",
    "                                  lstm_size,\n",
    "                                  max_enc, \n",
    "                                  max_dec, \n",
    "                                  'general',\n",
    "                                  att_units)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nRhvKBBvfgET"
   },
   "outputs": [],
   "source": [
    "pred_model.compile(optimizer = 'Adam', loss = custom_lossfunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3kkdcLO4fgET",
    "outputId": "c08ef497-0611-4399-ff69-13c1f3be0d13"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x2a4a6c34130>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the previously trained model\n",
    "pred_model.load_weights('general')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZAmTFwkfgET"
   },
   "outputs": [],
   "source": [
    "def predict(input_sequence):\n",
    "    '''\n",
    "    A. Given input sentence, convert the sentence into integers using tokenizer used earlier\n",
    "    B. Pass the input_sequence to encoder. we get encoder_outputs, last time step hidden and cell state\n",
    "    C. Initialize index of <start> as input to decoder. and encoder final states as input_states to onestepdecoder.\n",
    "    D. till we reach max_length of decoder or till the model predicted word <end>:\n",
    "         predictions, input_states, attention_weights = model.layers[1].onestepdecoder(input_to_decoder, encoder_output, input_states)\n",
    "         Save the attention weights\n",
    "         And get the word using the tokenizer(word index) and then store it in a string.\n",
    "    E. Call plot_attention(#params)\n",
    "    F. Return the predicted sentence\n",
    "    '''\n",
    "    \n",
    "    seq = input_sequence\n",
    "    seq = '<sos> '+seq+' <eos>'\n",
    "    seq = tokenizer_enc.texts_to_sequences([seq])\n",
    "    seq = pad_sequences(seq, maxlen=max_enc, padding='post', dtype = np.int32)\n",
    "    pred, attention_weights = pred_model.predict(tf.expand_dims(seq, 0))\n",
    "    output = []\n",
    "    for i in pred:\n",
    "        word = tokenizer_dec.index_word[i[0]]\n",
    "        if word == '<eos>':\n",
    "            break\n",
    "        output.append(word)\n",
    "    return ' '.join(output)  #, np.squeeze(np.squeeze(np.array(attention_weights), 1), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W_YgocTIfgET",
    "outputId": "d966aa5c-d562-47ac-d6db-b666956a12ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  Your support of\n",
      "predicted output :  the following agreement\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Your support of'\n",
    "print('input : ', sentence)\n",
    "\n",
    "result = predict(sentence)\n",
    "\n",
    "print('predicted output : ',result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q54zcHb0fgEU",
    "outputId": "4d4b9e09-6861-45d6-c5a4-5ff6f242d108"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  I also am including a draft of \n",
      "predicted output :  the following i will be able to be able to be able to be in the same\n"
     ]
    }
   ],
   "source": [
    "sentence = 'I also am including a draft of '\n",
    "print('input : ', sentence)\n",
    "\n",
    "result = predict(sentence)\n",
    "print('predicted output : ',result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYYF6t1rfgEU"
   },
   "source": [
    "### Bleu score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQBk1vCKfgEU"
   },
   "outputs": [],
   "source": [
    "def BLEUScore():\n",
    "    BLEUscore_list = []\n",
    "    \n",
    "    for x in  test['body_enc_seq'].sample(10000):\n",
    "        reference = x.split()\n",
    "        hypothesis = predict(x)\n",
    "        BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis.split())\n",
    "        BLEUscore_list.append(BLEUscore)\n",
    "        \n",
    "    return sum(BLEUscore_list)/len(BLEUscore_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dLgI8F23fgEU",
    "outputId": "3291fc50-965b-4c5d-dfe9-03fbda81b1e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.040189947624632516\n"
     ]
    }
   ],
   "source": [
    "bleuscore_lstm =BLEUScore()\n",
    "\n",
    "print(bleuscore_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_FCLlR1fgEU"
   },
   "source": [
    "# 2.3 Attention with Concat score function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-KX_-FTFfgEU"
   },
   "outputs": [],
   "source": [
    "#Compile and train your model on general scoring function.\n",
    "# Visualize few sentences randomly in Test data\n",
    "# Predict on 1000 random sentences on test data and calculate the average BLEU score of these sentences.\n",
    "# https://www.nltk.org/_modules/nltk/translate/bleu_score.html\n",
    "\n",
    "\n",
    "batch_size=32\n",
    "lstm_size=128\n",
    "max_dec = 50\n",
    "max_enc = 29\n",
    "embedding_dim = 100\n",
    "dense_units = 256\n",
    "att_units = 256\n",
    "latent_dim=192\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fOKJ8BhYfgEU"
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train, tokenizer_enc, tokenizer_dec, max_enc, max_dec)\n",
    "test_dataset  = Dataset(test, tokenizer_enc, tokenizer_dec, max_enc, max_dec)\n",
    "val_dataset  = Dataset(val, tokenizer_enc, tokenizer_dec, max_enc, max_dec)\n",
    "\n",
    "train_dataloader = Dataloder(train_dataset, batch_size=batch_size)\n",
    "test_dataloader = Dataloder(test_dataset, batch_size=batch_size)\n",
    "val_dataloader = Dataloder(val_dataset, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yE-0vxa5fgEV"
   },
   "outputs": [],
   "source": [
    "model = encoder_decoder(vocab_size_enc, \n",
    "                        vocab_size_dec, \n",
    "                        embedding_dim, \n",
    "                        lstm_size,\n",
    "                        lstm_size,\n",
    "                        max_enc, \n",
    "                        max_dec,\n",
    "                        'concat', \n",
    "                        att_units, \n",
    "                        batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E4T9QuDYfgEV"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'Adam', loss = custom_lossfunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KeTs1sxKfgEV"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "log_dir=\"logs/fit/concat/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "#A callback is a function passed as an argument to another function. This technique allows a function to call another function.\n",
    "callbacks = [ModelCheckpoint('concat', save_best_only= True, verbose = 1),\n",
    "             TensorBoard(log_dir = log_dir, histogram_freq=1, write_graph=True),\n",
    "             EarlyStopping(monitor='val_loss', patience = 5, verbose = 1),\n",
    "             ReduceLROnPlateau(monitor='val_loss', patience = 2, verbose = 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zlMq3B_9fgEV"
   },
   "outputs": [],
   "source": [
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0T8-o-ffgEV"
   },
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Wz8t6tbfgEV",
    "outputId": "edf6b9c4-29bf-43f5-9aa7-bfde72b72d08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 558s 511ms/step - loss: 1.3998 - val_loss: 1.2596\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.25955, saving model to concat\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 403s 403ms/step - loss: 1.2092 - val_loss: 1.1778\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.25955 to 1.17777, saving model to concat\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 402s 402ms/step - loss: 1.1380 - val_loss: 1.1180\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.17777 to 1.11803, saving model to concat\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 410s 410ms/step - loss: 1.0674 - val_loss: 1.0661\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.11803 to 1.06612, saving model to concat\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 472s 472ms/step - loss: 1.0158 - val_loss: 1.0233\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.06612 to 1.02334, saving model to concat\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 480s 480ms/step - loss: 0.9654 - val_loss: 0.9878\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.02334 to 0.98784, saving model to concat\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 454s 454ms/step - loss: 0.9414 - val_loss: 0.9547\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.98784 to 0.95471, saving model to concat\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 454s 454ms/step - loss: 0.8933 - val_loss: 0.9234\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.95471 to 0.92343, saving model to concat\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 461s 461ms/step - loss: 0.8615 - val_loss: 0.8967\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.92343 to 0.89672, saving model to concat\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 465s 465ms/step - loss: 0.8360 - val_loss: 0.8715\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.89672 to 0.87152, saving model to concat\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 437s 437ms/step - loss: 0.8190 - val_loss: 0.8481\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.87152 to 0.84807, saving model to concat\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 469s 469ms/step - loss: 0.7827 - val_loss: 0.8251\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.84807 to 0.82509, saving model to concat\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 473s 473ms/step - loss: 0.7623 - val_loss: 0.8031\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.82509 to 0.80315, saving model to concat\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 466s 466ms/step - loss: 0.7350 - val_loss: 0.7822\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.80315 to 0.78220, saving model to concat\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 468s 468ms/step - loss: 0.7260 - val_loss: 0.7638\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.78220 to 0.76383, saving model to concat\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 463s 463ms/step - loss: 0.7009 - val_loss: 0.7449\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.76383 to 0.74485, saving model to concat\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 458s 458ms/step - loss: 0.6826 - val_loss: 0.7269\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.74485 to 0.72691, saving model to concat\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 478s 478ms/step - loss: 0.6684 - val_loss: 0.7111\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.72691 to 0.71112, saving model to concat\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 464s 464ms/step - loss: 0.6372 - val_loss: 0.6959\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.71112 to 0.69595, saving model to concat\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 481s 481ms/step - loss: 0.6208 - val_loss: 0.6816\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.69595 to 0.68159, saving model to concat\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 486s 486ms/step - loss: 0.6052 - val_loss: 0.6671\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.68159 to 0.66710, saving model to concat\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 472s 472ms/step - loss: 0.5914 - val_loss: 0.6549\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.66710 to 0.65487, saving model to concat\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 472s 472ms/step - loss: 0.5769 - val_loss: 0.6415\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.65487 to 0.64154, saving model to concat\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 480s 480ms/step - loss: 0.5678 - val_loss: 0.6299\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.64154 to 0.62986, saving model to concat\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 476s 476ms/step - loss: 0.5509 - val_loss: 0.6185\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.62986 to 0.61845, saving model to concat\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 478s 478ms/step - loss: 0.5426 - val_loss: 0.6061\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.61845 to 0.60608, saving model to concat\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 486s 486ms/step - loss: 0.5260 - val_loss: 0.5948\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.60608 to 0.59478, saving model to concat\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 467s 467ms/step - loss: 0.5157 - val_loss: 0.5845\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.59478 to 0.58454, saving model to concat\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 444s 444ms/step - loss: 0.5041 - val_loss: 0.5760\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.58454 to 0.57602, saving model to concat\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 487s 487ms/step - loss: 0.4876 - val_loss: 0.5670\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.57602 to 0.56696, saving model to concat\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 2749s 3s/step - loss: 0.4801 - val_loss: 0.5577\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.56696 to 0.55772, saving model to concat\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 1676s 2s/step - loss: 0.4703 - val_loss: 0.5491\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.55772 to 0.54912, saving model to concat\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 1313s 1s/step - loss: 0.4619 - val_loss: 0.5410\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.54912 to 0.54099, saving model to concat\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 1129s 1s/step - loss: 0.4505 - val_loss: 0.5323\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.54099 to 0.53231, saving model to concat\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 1278s 1s/step - loss: 0.4427 - val_loss: 0.5250\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.53231 to 0.52502, saving model to concat\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 467s 467ms/step - loss: 0.4298 - val_loss: 0.5181\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.52502 to 0.51808, saving model to concat\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 465s 465ms/step - loss: 0.4236 - val_loss: 0.5103\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.51808 to 0.51031, saving model to concat\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 458s 458ms/step - loss: 0.4171 - val_loss: 0.5048\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.51031 to 0.50478, saving model to concat\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 449s 449ms/step - loss: 0.4057 - val_loss: 0.4988\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.50478 to 0.49883, saving model to concat\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 454s 454ms/step - loss: 0.4063 - val_loss: 0.4930\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.49883 to 0.49303, saving model to concat\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 457s 457ms/step - loss: 0.3881 - val_loss: 0.4849\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.49303 to 0.48492, saving model to concat\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 454s 454ms/step - loss: 0.3832 - val_loss: 0.4795\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.48492 to 0.47947, saving model to concat\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 461s 461ms/step - loss: 0.3837 - val_loss: 0.4742\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.47947 to 0.47423, saving model to concat\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 463s 463ms/step - loss: 0.3731 - val_loss: 0.4675\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.47423 to 0.46751, saving model to concat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 467s 467ms/step - loss: 0.3663 - val_loss: 0.4638\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.46751 to 0.46377, saving model to concat\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 469s 469ms/step - loss: 0.3616 - val_loss: 0.4589\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.46377 to 0.45892, saving model to concat\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 468s 468ms/step - loss: 0.3563 - val_loss: 0.4543\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.45892 to 0.45429, saving model to concat\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 470s 470ms/step - loss: 0.3542 - val_loss: 0.4489\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.45429 to 0.44890, saving model to concat\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 468s 468ms/step - loss: 0.3490 - val_loss: 0.4448\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.44890 to 0.44485, saving model to concat\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 468s 468ms/step - loss: 0.3334 - val_loss: 0.4417\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.44485 to 0.44172, saving model to concat\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x26b9e389d30>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model training\n",
    "model.fit(x = train_dataloader, \n",
    "          steps_per_epoch = 1000, #train_dataloader.__len__(),\n",
    "          validation_data = val_dataloader,\n",
    "          validation_steps = 100, #val_dataloader.__len__(),\n",
    "          epochs = 50,\n",
    "          verbose = 1,\n",
    "          callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UXqe-iqMfgEV",
    "outputId": "97f587c3-00b7-4a39-e834-636966b3290b",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 11236), started 0:00:50 ago. (Use '!kill 11236' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-edad658c88aee1cb\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-edad658c88aee1cb\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir logs/fit/concat/20220809-010708/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5j0mHKqyfgEV",
    "outputId": "2eb9d654-cb55-41e9-de0a-2ab1e9626a6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 44s 110ms/step - loss: 0.4230\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.42302584648132324"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_steps = 400 #test_dataloader.__len__()\n",
    "model.evaluate(test_dataloader,steps=test_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHMKlGhPfgEW"
   },
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aVWp_nLbfgEW"
   },
   "outputs": [],
   "source": [
    "class pred_Encoder_decoder(tf.keras.Model): \n",
    "    def __init__(self, inp_vocab_size, out_vocab_size, embedding_dim, enc_units, dec_units, max_ita, max_eng, score_fun, att_units):\n",
    "        #Intialize objects from encoder decoder\n",
    "        super(pred_Encoder_decoder, self).__init__()\n",
    "        self.encoder = Encoder(inp_vocab_size, embedding_dim, enc_units, max_ita)\n",
    "        self.one_step_decoder = One_Step_Decoder(out_vocab_size, embedding_dim, max_eng, dec_units ,score_fun ,att_units)\n",
    "        self.batch_size = batch_size\n",
    "    def call(self, params):\n",
    "        enc_inp = params[0]\n",
    "        initial_state = self.encoder.initialize_states(1)\n",
    "        output_state, enc_h, enc_c = self.encoder(enc_inp, initial_state)\n",
    "        pred = tf.expand_dims([tokenizer_dec.word_index['<sos>']], 0)\n",
    "        dec_h = enc_h\n",
    "        dec_c = enc_c\n",
    "        all_pred = []\n",
    "        all_attention = []\n",
    "        for t in range(50):  \n",
    "            pred, dec_h,dec_c, attention, _ = self.one_step_decoder(pred, output_state, dec_h, dec_c)\n",
    "            pred = tf.argmax(pred, axis = -1)\n",
    "            all_pred.append(pred)\n",
    "            pred = tf.expand_dims(pred, 0)\n",
    "            all_attention.append(attention)\n",
    "        return all_pred, all_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xp4jOgpLfgEW"
   },
   "outputs": [],
   "source": [
    "pred_model = pred_Encoder_decoder(vocab_size_enc, \n",
    "                                  vocab_size_dec, \n",
    "                                  embedding_dim, \n",
    "                                  lstm_size,\n",
    "                                  lstm_size,\n",
    "                                  max_enc, \n",
    "                                  max_dec, \n",
    "                                  'concat',\n",
    "                                  att_units)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GO1KNd_rfgEW"
   },
   "outputs": [],
   "source": [
    "pred_model.compile(optimizer = 'Adam', loss = custom_lossfunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hi-ANkmefgEW",
    "outputId": "c08ef497-0611-4399-ff69-13c1f3be0d13"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x26d0fe983d0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the previously trained model\n",
    "pred_model.load_weights('concat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FKakn-q9fgEW"
   },
   "outputs": [],
   "source": [
    "def predict(input_sequence):\n",
    "    '''\n",
    "    A. Given input sentence, convert the sentence into integers using tokenizer used earlier\n",
    "    B. Pass the input_sequence to encoder. we get encoder_outputs, last time step hidden and cell state\n",
    "    C. Initialize index of <start> as input to decoder. and encoder final states as input_states to onestepdecoder.\n",
    "    D. till we reach max_length of decoder or till the model predicted word <end>:\n",
    "         predictions, input_states, attention_weights = model.layers[1].onestepdecoder(input_to_decoder, encoder_output, input_states)\n",
    "         Save the attention weights\n",
    "         And get the word using the tokenizer(word index) and then store it in a string.\n",
    "    E. Call plot_attention(#params)\n",
    "    F. Return the predicted sentence\n",
    "    '''\n",
    "    \n",
    "    seq = input_sequence\n",
    "    seq = '<sos> '+seq+' <eos>'\n",
    "    seq = tokenizer_enc.texts_to_sequences([seq])\n",
    "    seq = pad_sequences(seq, maxlen=max_enc, padding='post', dtype = np.int32)\n",
    "    pred, attention_weights = pred_model.predict(tf.expand_dims(seq, 0))\n",
    "    output = []\n",
    "    for i in pred:\n",
    "        word = tokenizer_dec.index_word[i[0]]\n",
    "        if word == '<eos>':\n",
    "            break\n",
    "        output.append(word)\n",
    "    return ' '.join(output)  #, np.squeeze(np.squeeze(np.array(attention_weights), 1), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZTZUqPVWfgEX",
    "outputId": "d966aa5c-d562-47ac-d6db-b666956a12ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  Your support of\n",
      "predicted output :  the end of hurricane season buyer\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Your support of'\n",
    "print('input : ', sentence)\n",
    "\n",
    "result = predict(sentence)\n",
    "\n",
    "print('predicted output : ',result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qw21armifgEX",
    "outputId": "4d4b9e09-6861-45d6-c5a4-5ff6f242d108"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input :  I also am including a draft of \n",
      "predicted output :  the new date of a letter of august th\n"
     ]
    }
   ],
   "source": [
    "sentence = 'I also am including a draft of '\n",
    "print('input : ', sentence)\n",
    "\n",
    "result = predict(sentence)\n",
    "print('predicted output : ',result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GP-75s9FfgEX"
   },
   "source": [
    "### Bleu score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7FtShF_efgEX"
   },
   "outputs": [],
   "source": [
    "def BLEUScore():\n",
    "    BLEUscore_list = []\n",
    "    \n",
    "    for x in  test['body_enc_seq'].sample(10000):\n",
    "        reference = x.split()\n",
    "        hypothesis = predict(x)\n",
    "        BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis.split())\n",
    "        BLEUscore_list.append(BLEUscore)\n",
    "        \n",
    "    return sum(BLEUscore_list)/len(BLEUscore_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BE5F-DvufgEX",
    "outputId": "3291fc50-965b-4c5d-dfe9-03fbda81b1e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5276267049723287\n"
     ]
    }
   ],
   "source": [
    "bleuscore_lstm =BLEUScore()\n",
    "\n",
    "print(bleuscore_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gJgsfkI-fgEX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EoZrs6ir5eGi"
   },
   "source": [
    "#  PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GN4Y3kEI5eGi",
    "outputId": "9bd99823-51ac-478f-cf9b-d599573bbbfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+------------+-----------------+-----------+------------+\n",
      "| Model Without Attention layer | Train loss | Validation loss | Test loss | Bleu Score |\n",
      "+-------------------------------+------------+-----------------+-----------+------------+\n",
      "|    LSTM with Adam Optimizer   |   0.1553   |      0.3549     |   0.3542  |    0.48    |\n",
      "|  LSTM with RMSProp Optimizer  |   0.5640   |      0.6775     |   0.6797  |    0.39    |\n",
      "+-------------------------------+------------+-----------------+-----------+------------+\n",
      " --------------------------------------------------------------------------------------------- \n",
      "+---------------------------------------+------------+-----------------+-----------+------------+\n",
      "|       Model With Attention layer      | Train loss | Validation loss | Test loss | Bleu Score |\n",
      "+---------------------------------------+------------+-----------------+-----------+------------+\n",
      "|   Attention with Dot score function   |   0.3583   |      0.4604     |   0.4366  |    0.49    |\n",
      "| Attention with General score function |   0.8885   |      0.9564     |   0.9184  |    0.04    |\n",
      "|  Attention with Concat score function |   0.3334   |      0.4417     |   0.4230  |    0.52    |\n",
      "+---------------------------------------+------------+-----------------+-----------+------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "# Specify the Column Names while initializing the Table\n",
    "wo = PrettyTable([\"Model Without Attention layer\", \"Train loss\", \"Validation loss\", \"Test loss\", \"Bleu Score\"])\n",
    "# Add rows\n",
    "wo.add_row([\"LSTM with Adam Optimizer\", \"0.1553\", \"0.3549\", \"0.3542\", \"0.48\"])\n",
    "wo.add_row([\"LSTM with RMSProp Optimizer\", \"0.5640\", \"0.6775\", \"0.6797\",\"0.39\"])\n",
    "print(wo)\n",
    "\n",
    "print(\" --------------------------------------------------------------------------------------------- \")\n",
    "\n",
    "# Specify the Column Names while initializing the Table\n",
    "w = PrettyTable([\"Model With Attention layer\", \"Train loss\", \"Validation loss\", \"Test loss\", \"Bleu Score\"])\n",
    "# Add rows\n",
    "w.add_row([\"Attention with Dot score function\", \"0.3583\", \"0.4604\",\"0.4366\" ,\"0.49\"])\n",
    "w.add_row([\"Attention with General score function\", \"0.8885\", \"0.9564\", \"0.9184\",\"0.04\"])\n",
    "w.add_row([\"Attention with Concat score function\", \"0.3334\", \"0.4417\", \"0.4230\" ,\"0.52\"])\n",
    "print(w)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Ah5yUKQ5eGi"
   },
   "source": [
    "# Conclsion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2niaZj95eGi"
   },
   "source": [
    "1. We got better train and validation loss results when we used ADAM optimizer with the LSTM model. But for better perfection we can consider Attention with Concat score function model as better model.\n",
    "\n",
    "\n",
    "2. We have also calculated the Bleu score on the top of the randomly picked 10000 test data sample, Over there we also got the same result.\n",
    "\n",
    "\n",
    "3. In the final modelling part, we will use the Attention with Concat score function model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ldsgkqEUfgEY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "Wd8VStBI5eGb",
    "RASnO12t5eGb",
    "vCm_B_Iz5eGb",
    "rIOq1ZN75eGc",
    "EbH6-Omx5eGe",
    "70YWBdSI5eGf",
    "m_am_RDX5eGf",
    "WXTgKMSW5eGg",
    "b1Dut1aH5eGi",
    "EoZrs6ir5eGi",
    "6Ah5yUKQ5eGi"
   ],
   "name": "Model_v.2.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
